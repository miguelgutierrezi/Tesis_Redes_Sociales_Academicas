{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdIGXR6VaA9e"
      },
      "source": [
        "# Instalación de librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AENgY1SCnt0",
        "outputId": "58a8cb2d-2a3a-4056-de7c-dbc70a943e22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mcli\u001b[0m npm v9.6.7 does not support Node.js v14.16.0. This version of npm supports the following node versions: `^14.17.0 || ^16.13.0 || >=18.0.0`. You can find the latest version at https://nodejs.org/.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35mcode\u001b[0m EBADENGINE\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35mengine\u001b[0m Unsupported engine\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35mengine\u001b[0m Not compatible with your version of node/npm: npm@9.6.7\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m Not compatible with your version of node/npm: npm@9.6.7\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m Required: {\"node\":\"^14.17.0 || ^16.13.0 || >=18.0.0\"}\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m Actual:   {\"npm\":\"9.6.7\",\"node\":\"v14.16.0\"}\n",
            "\u001b[0m\n",
            "\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m A complete log of this run can be found in: /root/.npm/_logs/2023-06-01T01_55_28_345Z-debug-0.log\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mcli\u001b[0m npm v9.6.7 does not support Node.js v14.16.0. This version of npm supports the following node versions: `^14.17.0 || ^16.13.0 || >=18.0.0`. You can find the latest version at https://nodejs.org/.\n",
            "\u001b[K\u001b[?25h\n",
            "up to date, audited 23 packages in 530ms\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[32m\u001b[1m0\u001b[22m\u001b[39m vulnerabilities\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyvis in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.8.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis) (7.34.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.1.2)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.0.1)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.1)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (8.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.10/dist-packages (from geopandas) (1.9.4.post1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas) (23.1)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from geopandas) (1.5.3)\n",
            "Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from geopandas) (3.5.0)\n",
            "Requirement already satisfied: shapely>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from geopandas) (2.0.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (23.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (2022.12.7)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (0.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (1.16.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.18.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9.6->pyvis) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas) (2022.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.0.9)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.10/dist-packages (from geopandas) (1.9.4.post1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas) (23.1)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from geopandas) (1.5.3)\n",
            "Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from geopandas) (3.5.0)\n",
            "Requirement already satisfied: shapely>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from geopandas) (2.0.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (23.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (2022.12.7)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (8.1.3)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (0.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: gdal in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit\n",
        "!npm install -g npm\n",
        "!npm install localtunnel\n",
        "!pip install pyvis wordcloud nltk geopandas gensim\n",
        "!pip install geopandas\n",
        "!pip install pandas numpy shapely gdal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbFqU0gIUEtY",
        "outputId": "af5f3a8c-2567-4442-98ec-fd61ec346b29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "stop_words_english = nltk.corpus.stopwords.words('english')\n",
        "stemmer = SnowballStemmer('english')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2rGtQWJEj1X"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YIAwM3laTnA"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeyI8HgZqbwU",
        "outputId": "4c1264d9-ecc1-41bc-9710-fba2737046e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting EDA.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile EDA.py\n",
        "\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "\n",
        "# Perform exploratory data analysis (EDA)\n",
        "def perform_eda(data, st):\n",
        "\n",
        "    # Mostrar los datos\n",
        "    st.subheader(\"Datos Previos\")\n",
        "    st.dataframe(data.head())\n",
        "\n",
        "    # Número de registros\n",
        "    st.write(\"Número de observaciones:\", data.shape[0])\n",
        "    st.write(\"Número de variables:\", data.shape[1])\n",
        "\n",
        "    # Estadísticas resumidas\n",
        "    st.write(\"Resumen estadístico:\")\n",
        "    st.write(data.describe(include='all'))\n",
        "\n",
        "    # Valores faltantes\n",
        "    st.write(\"Valores faltantes:\")\n",
        "    conteo_valores_faltantes = data.isnull().sum()\n",
        "    porcentaje_valores_faltantes = (conteo_valores_faltantes / len(data)) * 100\n",
        "\n",
        "    valores_faltantes_df = pd.DataFrame({\n",
        "        \"Columna\": conteo_valores_faltantes.index,\n",
        "        \"Conteo de Faltantes\": conteo_valores_faltantes.values,\n",
        "        \"Porcentaje de Faltantes\": [f\"{porcentaje:.2f}%\" for porcentaje in porcentaje_valores_faltantes.values]\n",
        "    })\n",
        "    st.dataframe(valores_faltantes_df)\n",
        "\n",
        "    # Análisis adicional de EDA y visualizaciones\n",
        "    ## Value counts para 'Patrocinadores'\n",
        "    #st.subheader(\"Top 10 de patrocinadores:\")\n",
        "    #top_fund = data['fund_sponsor'].value_counts().head(10)\n",
        "    #st.write(top_fund)\n",
        "\n",
        "    # Export nodes to CSV if export button is clicked\n",
        "    top_fund_export_button = st.button(\"Exportar a CSV patrocinadores\")\n",
        "    if top_fund_export_button:\n",
        "        # CSV filename\n",
        "        csv_filename = \"top_fund_sponsor.csv\"\n",
        "\n",
        "        # Write different nodes to CSV file\n",
        "        with open(csv_filename, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Nodo\"])  # Write header\n",
        "            writer.writerows(zip(list(top_fund)))  # Write nodes\n",
        "\n",
        "        # Success message\n",
        "        st.success(f\"Se ha exportado la lista de patrocinadores a {csv_filename}\")\n",
        "\n",
        "    # Value counts para 'Authors'\n",
        "    st.subheader(\"Top 10 de autores más frecuentes:\")\n",
        "    top_autores = data['Authors'].value_counts().head(10)\n",
        "    st.write(top_autores)\n",
        "\n",
        "    # Export nodes to CSV if export button is clicked\n",
        "    top_authors_export_button = st.button(\"Exportar a CSV autores\")\n",
        "    if top_authors_export_button:\n",
        "        # CSV filename\n",
        "        csv_filename = \"top_authors.csv\"\n",
        "\n",
        "        # Write different nodes to CSV file\n",
        "        with open(csv_filename, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Nodo\"])  # Write header\n",
        "            writer.writerows(zip(list(top_autores)))  # Write nodes\n",
        "\n",
        "        # Success message\n",
        "        st.success(f\"Se ha exportado la lista de autores a {csv_filename}\")\n",
        "\n",
        "    # Value counts para 'Affiliations'\n",
        "    st.subheader(\"Top 10 de Afiliaciones más frecuentes:\")\n",
        "    top_Affiliations = data['Affiliations'].value_counts().head(10)\n",
        "    st.write(top_Affiliations)\n",
        "    \n",
        "    # Export nodes to CSV if export button is clicked\n",
        "    top_affiliations_export_button = st.button(\"Exportar a CSV afiliaciones\")\n",
        "    if top_authors_export_button:\n",
        "        # CSV filename\n",
        "        csv_filename = \"top_affiliations.csv\"\n",
        "\n",
        "        # Write different nodes to CSV file\n",
        "        with open(csv_filename, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Nodo\"])  # Write header\n",
        "            writer.writerows(zip(list(top_Affiliations)))  # Write nodes\n",
        "\n",
        "        # Success message\n",
        "        st.success(f\"Se ha exportado la lista de afiliaciones a {csv_filename}\")\n",
        "\n",
        "    # Value counts para 'Year'\n",
        "    st.subheader(\"Ranking de publicaciones por año:\")\n",
        "    value_counts_year = data['Year'].value_counts()\n",
        "    st.write(value_counts_year)\n",
        "\n",
        "    # Export nodes to CSV if export button is clicked\n",
        "    top_value_counts_export_button = st.button(\"Exportar a CSV publicaciones por año\")\n",
        "    if top_value_counts_export_button:\n",
        "        # CSV filename\n",
        "        csv_filename = \"top_value_counts.csv\"\n",
        "\n",
        "        # Write different nodes to CSV file\n",
        "        with open(csv_filename, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Nodo\"])  # Write header\n",
        "            writer.writerows(zip(list(value_counts_year)))  # Write nodes\n",
        "\n",
        "        # Success message\n",
        "        st.success(f\"Se ha exportado el ranking de publicaciones a {csv_filename}\")\n",
        "\n",
        "    # Value counts para 'country'\n",
        "    st.subheader(\"Top 10 de ciudades más frecuentes:\")\n",
        "    top_country = data['country'].value_counts().head(10)\n",
        "    st.write(top_country)\n",
        "\n",
        "    # Export nodes to CSV if export button is clicked\n",
        "    top_country_export_button = st.button(\"Exportar a CSV ciudades\")\n",
        "    if top_country_export_button:\n",
        "        # CSV filename\n",
        "        csv_filename = \"top_country.csv\"\n",
        "\n",
        "        # Write different nodes to CSV file\n",
        "        with open(csv_filename, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Nodo\"])  # Write header\n",
        "            writer.writerows(zip(list(top_country)))  # Write nodes\n",
        "\n",
        "        # Success message\n",
        "        st.success(f\"Se ha exportado el top de ciudades a {csv_filename}\")\n",
        "\n",
        "    # Value counts para 'institution'\n",
        "    st.subheader(\"Top 10 de instituciones más frecuentes:\")\n",
        "    top_institution = data['institution'].value_counts().head(10)\n",
        "    st.write(top_institution)\n",
        "\n",
        "    # Export nodes to CSV if export button is clicked\n",
        "    top_institution_export_button = st.button(\"Exportar a CSV instituciones\")\n",
        "    if top_institution_export_button:\n",
        "        # CSV filename\n",
        "        csv_filename = \"top_institution.csv\"\n",
        "\n",
        "        # Write different nodes to CSV file\n",
        "        with open(csv_filename, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Nodo\"])  # Write header\n",
        "            writer.writerows(zip(list(top_institution)))  # Write nodes\n",
        "\n",
        "        # Success message\n",
        "        st.success(f\"Se ha exportado el top de instituciones a {csv_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iajlld1V3EA6"
      },
      "source": [
        "# EDA Proyectos de investigación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C4TOOi_3Hqv",
        "outputId": "6468b68f-8589-4088-fe7e-5bb9ea4d634a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting EDA_proy_inv.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile EDA_proy_inv.py\n",
        "\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Perform exploratory data analysis (EDA)\n",
        "def perform_eda_proy_inv(st):\n",
        "    data = pd.read_csv('projects.csv')\n",
        "    # Mostrar los datos\n",
        "    st.subheader(\"Datos Previos\")\n",
        "    st.dataframe(data.head())\n",
        "\n",
        "    # Número de registros\n",
        "    st.write(\"Número de observaciones:\", data.shape[0])\n",
        "    st.write(\"Número de variables:\", data.shape[1])\n",
        "\n",
        "    # Estadísticas resumidas\n",
        "    st.write(\"Resumen estadístico:\")\n",
        "    st.write(data.describe(include='all'))\n",
        "\n",
        "    # Valores faltantes\n",
        "    st.write(\"Valores faltantes:\")\n",
        "    conteo_valores_faltantes = data.isnull().sum()\n",
        "    porcentaje_valores_faltantes = (conteo_valores_faltantes / len(data)) * 100\n",
        "\n",
        "    valores_faltantes_df = pd.DataFrame({\n",
        "        \"Columna\": conteo_valores_faltantes.index,\n",
        "        \"Conteo de Faltantes\": conteo_valores_faltantes.values,\n",
        "        \"Porcentaje de Faltantes\": [f\"{porcentaje:.2f}%\" for porcentaje in porcentaje_valores_faltantes.values]\n",
        "    })\n",
        "    st.dataframe(valores_faltantes_df)\n",
        "\n",
        "    # Análisis adicional de EDA y visualizaciones\n",
        "    st.subheader(\"Análisis de algunas variables relevantes\")\n",
        "\n",
        "    # Value counts para 'Authors'\n",
        "    st.write(\"Top 10 de autores más frecuentes:\")\n",
        "    top_autores = data['autores'].value_counts().head(10)\n",
        "    st.write(top_autores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NSVmSRrB4aQN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-ngkybbovO3J"
      },
      "source": [
        "# EDA WoS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk-yHYFwvO3K",
        "outputId": "e35cbd66-0ef0-4af4-8e8e-c1182155f109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting EDA_wos.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile EDA_wos.py\n",
        "\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Perform exploratory data analysis (EDA)\n",
        "def perform_eda_wos(st):\n",
        "    data = pd.read_csv('wos.csv')\n",
        "    # Mostrar los datos\n",
        "    st.subheader(\"Datos Previos\")\n",
        "    st.dataframe(data.head())\n",
        "\n",
        "    # Número de registros\n",
        "    st.write(\"Número de observaciones:\", data.shape[0])\n",
        "    st.write(\"Número de variables:\", data.shape[1])\n",
        "\n",
        "    # Estadísticas resumidas\n",
        "    st.write(\"Resumen estadístico:\")\n",
        "    st.write(data.describe(include='all'))\n",
        "\n",
        "    # Valores faltantes\n",
        "    st.write(\"Valores faltantes:\")\n",
        "    conteo_valores_faltantes = data.isnull().sum()\n",
        "    porcentaje_valores_faltantes = (conteo_valores_faltantes / len(data)) * 100\n",
        "\n",
        "    valores_faltantes_df = pd.DataFrame({\n",
        "        \"Columna\": conteo_valores_faltantes.index,\n",
        "        \"Conteo de Faltantes\": conteo_valores_faltantes.values,\n",
        "        \"Porcentaje de Faltantes\": [f\"{porcentaje:.2f}%\" for porcentaje in porcentaje_valores_faltantes.values]\n",
        "    })\n",
        "    st.dataframe(valores_faltantes_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yun3jXwp4lKd"
      },
      "source": [
        "# EDA Scopus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWD5xT074fEp",
        "outputId": "1b245ccd-f10f-48f7-e453-39659989e452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting EDA_scopus.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile EDA_scopus.py\n",
        "\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "\n",
        "# Perform exploratory data analysis (EDA)\n",
        "def perform_eda_scopus(st):\n",
        "\n",
        "    data = pd.read_csv('Collection_Correct_Name_Scopus.csv')\n",
        "\n",
        "    # Mostrar los datos\n",
        "    st.subheader(\"Datos Previos\")\n",
        "    st.dataframe(data.head())\n",
        "\n",
        "    # Número de registros\n",
        "    st.write(\"Número de observaciones:\", data.shape[0])\n",
        "    st.write(\"Número de variables:\", data.shape[1])\n",
        "\n",
        "    # Estadísticas resumidas\n",
        "    st.write(\"Resumen estadístico:\")\n",
        "    st.write(data.describe(include='all'))\n",
        "\n",
        "    # Valores faltantes\n",
        "    st.write(\"Valores faltantes:\")\n",
        "    conteo_valores_faltantes = data.isnull().sum()\n",
        "    porcentaje_valores_faltantes = (conteo_valores_faltantes / len(data)) * 100\n",
        "\n",
        "    valores_faltantes_df = pd.DataFrame({\n",
        "        \"Columna\": conteo_valores_faltantes.index,\n",
        "        \"Conteo de Faltantes\": conteo_valores_faltantes.values,\n",
        "        \"Porcentaje de Faltantes\": [f\"{porcentaje:.2f}%\" for porcentaje in porcentaje_valores_faltantes.values]\n",
        "    })\n",
        "    st.dataframe(valores_faltantes_df)\n",
        "\n",
        "    # Análisis adicional de EDA y visualizaciones\n",
        "    # Value counts para 'Patrocinadores'\n",
        "    st.subheader(\"Top 10 de los patrocinadores más frecuentes:\")\n",
        "    top_fund = data['fund_sponsor'].value_counts().head(10)\n",
        "    st.write(top_fund)\n",
        "\n",
        "    # Export nodes to CSV if export button is clicked\n",
        "    top_fund_export_button = st.button(\"Exportar a CSV patrocinadores\")\n",
        "    if top_fund_export_button:\n",
        "        # CSV filename\n",
        "        csv_filename = \"top_fund_sponsor.csv\"\n",
        "\n",
        "        # Write different nodes to CSV file\n",
        "        with open(csv_filename, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Nodo\"])  # Write header\n",
        "            writer.writerows(zip(list(top_fund)))  # Write nodes\n",
        "\n",
        "        # Success message\n",
        "        st.success(f\"Se ha exportado la lista de patrocinadores a {csv_filename}\")\n",
        "\n",
        "    # Value counts para 'citedby_count'\n",
        "    st.subheader(\"Top 10 de autor con más citas:\")\n",
        "    top_cite_count = data.groupby('creator')['citedby_count'].size().sort_values(ascending=False).reset_index()\n",
        "    top_cite_count.columns = ['Autor', 'Conteo Citas']\n",
        "    top_cite_count = top_cite_count.head(10)\n",
        "    st.write(top_cite_count)\n",
        "\n",
        "    # Export nodes to CSV if export button is clicked\n",
        "    top_authors_export_button = st.button(\"Exportar a CSV autores\")\n",
        "    if top_authors_export_button:\n",
        "        # CSV filename\n",
        "        csv_filename = \"top_authors.csv\"\n",
        "\n",
        "        # Write different nodes to CSV file\n",
        "        with open(csv_filename, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Nodo\"])  # Write header\n",
        "            writer.writerows(zip(list(top_cite_count)))  # Write nodes\n",
        "\n",
        "        # Success message\n",
        "        st.success(f\"Se ha exportado la lista de autores a {csv_filename}\")\n",
        "\n",
        "    ## Value counts para 'Affiliations'\n",
        "    st.subheader(\"Top 10 de Afiliaciones más frecuentes:\")\n",
        "    top_Affiliations = data['affilname'].value_counts().head(10)\n",
        "    st.write(top_Affiliations)\n",
        "\n",
        "    # Export nodes to CSV if export button is clicked\n",
        "    top_affiliations_export_button = st.button(\"Exportar a CSV afiliaciones\")\n",
        "    if top_authors_export_button:\n",
        "        # CSV filename\n",
        "        csv_filename = \"top_affiliations.csv\"\n",
        "\n",
        "        # Write different nodes to CSV file\n",
        "        with open(csv_filename, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Nodo\"])  # Write header\n",
        "            writer.writerows(zip(list(top_Affiliations)))  # Write nodes\n",
        "\n",
        "        # Success message\n",
        "        st.success(f\"Se ha exportado la lista de afiliaciones a {csv_filename}\")\n",
        "\n",
        "    # Value counts para 'country'\n",
        "    st.subheader(\"Top 10 de países más frecuentes:\")\n",
        "    top_country = data['affiliation_country'].value_counts().head(10)\n",
        "    st.write(top_country)\n",
        "\n",
        "    # Export nodes to CSV if export button is clicked\n",
        "    top_country_export_button = st.button(\"Exportar a CSV países\")\n",
        "    if top_country_export_button:\n",
        "        # CSV filename\n",
        "        csv_filename = \"top_country.csv\"\n",
        "\n",
        "        # Write different nodes to CSV file\n",
        "        with open(csv_filename, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Nodo\"])  # Write header\n",
        "            writer.writerows(zip(list(top_country)))  # Write nodes\n",
        "\n",
        "        # Success message\n",
        "        st.success(f\"Se ha exportado el top de ciudades a {csv_filename}\")\n",
        "\n",
        "    # Value counts para 'institution'\n",
        "    st.subheader(\"Top 10 de nombre de publicaciones más frecuentes:\")\n",
        "    top_institution = data['publicationName'].value_counts().head(10)\n",
        "    st.write(top_institution)\n",
        "\n",
        "    # Export nodes to CSV if export button is clicked\n",
        "    top_institution_export_button = st.button(\"Exportar a CSV instituciones\")\n",
        "    if top_institution_export_button:\n",
        "        # CSV filename\n",
        "        csv_filename = \"top_institution.csv\"\n",
        "\n",
        "        # Write different nodes to CSV file\n",
        "        with open(csv_filename, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Nodo\"])  # Write header\n",
        "            writer.writerows(zip(list(top_institution)))  # Write nodes\n",
        "\n",
        "        # Success message\n",
        "        st.success(f\"Se ha exportado el top de instituciones a {csv_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aEL2I2_aY7G"
      },
      "source": [
        "# Scopus Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5jffL7dtd61",
        "outputId": "ebc1907c-db3b-42d0-cf98-bf973453a429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scopus_network.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scopus_network.py\n",
        "\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvis.network import Network\n",
        "\n",
        "\n",
        "# Function to load the nodes and edges CSV files\n",
        "def load_data_network():\n",
        "    nodes_df = pd.read_csv('nodes.csv')\n",
        "    edges_df = pd.read_csv('edges.csv')\n",
        "    return nodes_df, edges_df\n",
        "\n",
        "\n",
        "# Function to create the network graph using NetworkX\n",
        "def create_graph(nodes_df, edges_df):\n",
        "    G = nx.Graph()\n",
        "    for _, row in nodes_df.iterrows():\n",
        "        G.add_node(row['Node'], ID=row['ID'])\n",
        "    for _, row in edges_df.iterrows():\n",
        "        G.add_edge(row['Source'], row['Target'])\n",
        "    return G\n",
        "\n",
        "\n",
        "# Function to get the first and second tier nodes for a given node\n",
        "def get_tiers(G, node):\n",
        "    first_tier = list(G.neighbors(node))\n",
        "    second_tier = []\n",
        "    for neighbor in first_tier:\n",
        "        second_tier.extend(list(G.neighbors(neighbor)))\n",
        "    second_tier = list(set(second_tier))  # Remove duplicates\n",
        "    return first_tier, second_tier\n",
        "\n",
        "\n",
        "# Create graph visualization based on selected attribute\n",
        "def visualization_graph(st):\n",
        "    # Load the data\n",
        "    nodes_df, edges_df = load_data_network()\n",
        "\n",
        "    # Create the graph\n",
        "    G = create_graph(nodes_df, edges_df)\n",
        "\n",
        "    # Dropdown to select a node\n",
        "    selected_node = st.selectbox(\"Seleccione el autor\", nodes_df['Node'])\n",
        "\n",
        "    # Get first and second tier nodes\n",
        "    first_tier_nodes, second_tier_nodes = get_tiers(G, selected_node)\n",
        "\n",
        "    # Subgraph containing the selected node and its first and second tier nodes\n",
        "    subgraph = G.subgraph([selected_node] + first_tier_nodes + second_tier_nodes)\n",
        "\n",
        "    # Visualization using pyvis\n",
        "    network = Network(width=\"100%\", height=\"800px\", notebook=True)\n",
        "    network.from_nx(subgraph)\n",
        "    network.show(\"network.html\")\n",
        "\n",
        "    # Display the network visualization\n",
        "    st.components.v1.html(open(\"network.html\", 'r', encoding='utf-8').read(), width=1000, height=600)\n",
        "\n",
        "    # Display first and second tier nodes\n",
        "    st.write(\"Coautores del investigador:\")\n",
        "    st.write(first_tier_nodes)\n",
        "    st.write(\"Coautores de los coautores:\")\n",
        "    st.write(second_tier_nodes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rauopxOe520w"
      },
      "source": [
        "# Afiliation Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHl3iq_s6Eb0",
        "outputId": "9aeca724-d034-4e21-b58a-d878f0f75642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting afi1_network.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile afi1_network.py\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvis.network import Network\n",
        "import csv\n",
        "import time\n",
        "\n",
        "\n",
        "# Function to load the nodes and edges CSV files\n",
        "def load_data_network():\n",
        "    nodes_df_afi = pd.read_csv('nodes60033370.csv')\n",
        "    edges_df_afi = pd.read_csv('edges60033370.csv')\n",
        "    return nodes_df_afi, edges_df_afi\n",
        "\n",
        "# Function to create the network graph using NetworkX\n",
        "def create_graph_afi(nodes_df_afi, edges_df_afi):\n",
        "    GA = nx.Graph()\n",
        "    for _, row in nodes_df_afi.iterrows():\n",
        "        GA.add_node(row['Node'], ID=row['ID'])\n",
        "    for _, row in edges_df_afi.iterrows():\n",
        "        GA.add_edge(row['Source'], row['Target'])\n",
        "    return GA\n",
        "\n",
        "# Function to get the first and second tier nodes for a given node\n",
        "def get_tiers(GA, node):\n",
        "    first_tier_afi = list(GA.neighbors(node))\n",
        "    second_tier_afi = []\n",
        "    for neighbor in first_tier_afi:\n",
        "        second_tier_afi.extend(list(GA.neighbors(neighbor)))\n",
        "    second_tier_afi = list(set(second_tier_afi))  # Remove duplicates\n",
        "    return first_tier_afi, second_tier_afi\n",
        "\n",
        "# Create graph visualization based on selected attribute\n",
        "def visualization_graph_afi(st):\n",
        "\n",
        "    # Load the data\n",
        "    nodes_df_afi, edges_df_afi = load_data_network()\n",
        "\n",
        "    # Create the graph\n",
        "    GA = create_graph_afi(nodes_df_afi, edges_df_afi)\n",
        "\n",
        "    # Dropdown to select a node\n",
        "    #selected_node = st.selectbox(\"Seleccione el autor\", nodes_df_afi['Node'])\n",
        "    selected_node = (\"Pontificia Universidad Javeriana\")\n",
        "\n",
        "    # Get first and second tier nodes\n",
        "    first_tier_afi_nodes, second_tier_afi_nodes = get_tiers(GA, selected_node)\n",
        "\n",
        "    # Subgraph containing the selected node and its first and second tier nodes\n",
        "    subgraph = GA.subgraph([selected_node] + first_tier_afi_nodes + second_tier_afi_nodes)\n",
        "\n",
        "    # Visualization using pyvis\n",
        "    network = Network(width=\"100%\", height=\"800px\", notebook=True)\n",
        "    network.from_nx(subgraph)\n",
        "    network.show(\"network_afi.html\")\n",
        "\n",
        "    # Display the network visualization\n",
        "    st.components.v1.html(open(\"network_afi.html\", 'r', encoding='utf-8').read(), width=1000, height=600)\n",
        "\n",
        "    # Display nodes different from the selected node\n",
        "    all_nodes = GA.nodes()\n",
        "    selected_node = set([selected_node])\n",
        "    different_nodes = set(all_nodes) - selected_node\n",
        "    st.write(\"universidades vinculadas\")\n",
        "    st.write(list(different_nodes))\n",
        "\n",
        "    # Export nodes to CSV if export button is clicked\n",
        "    export_button = st.button(\"Exportar a CSV\")\n",
        "    if export_button:\n",
        "        # CSV filename\n",
        "        csv_filename = \"VinculosUniversidades.csv\"\n",
        "\n",
        "        # Write different nodes to CSV file\n",
        "        with open(csv_filename, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Nodo\"])  # Write header\n",
        "            writer.writerows(zip(list(different_nodes)))  # Write nodes\n",
        "\n",
        "        # Success message\n",
        "        st.success(f\"Se ha exportado la lista de nodos diferentes a {csv_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX5sOu1sae1B"
      },
      "source": [
        "# Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16A1JMPvv27K",
        "outputId": "a7731c10-6cc3-46ac-8697-5fe985fcb5a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cluster.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile cluster.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "\n",
        "# Detect communities in the graph\n",
        "def detect_communities(data, st):\n",
        "    # Load the data\n",
        "    df = data\n",
        "\n",
        "    # Select relevant columns for clustering\n",
        "    data = df[['Abstract', 'Author Keywords', 'Index Keywords', 'Authors', 'Title']].dropna()\n",
        "\n",
        "    # Splitting the words by semicolon\n",
        "    data['Abstract'] = df['Abstract'].str.split(';')\n",
        "\n",
        "    # Flattening the list of words and applying filters\n",
        "    data['Abstract'] = data['Abstract'].apply(\n",
        "        lambda x: [re.sub(r'[^\\w\\s]', '', word.strip()) for word in x if len(word.strip()) > 4])\n",
        "\n",
        "    # Joining the words back into a single string\n",
        "    data['Abstract'] = data['Abstract'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Create TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(data['Abstract'])\n",
        "\n",
        "    # Elbow Method\n",
        "    inertia_values = []\n",
        "    silhouette_scores = []\n",
        "    max_clusters = 10\n",
        "\n",
        "    for n_clusters in range(2, max_clusters + 1):\n",
        "        # Apply K-means clustering\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        kmeans.fit(X)\n",
        "\n",
        "        # Get cluster labels\n",
        "        labels = kmeans.labels_\n",
        "        inertia = kmeans.inertia_\n",
        "        silhouette_avg = silhouette_score(X, labels)\n",
        "\n",
        "        inertia_values.append(inertia)\n",
        "        silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "    # Determine the best number of clusters using the Elbow Method\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    ax[0].plot(range(2, max_clusters + 1), inertia_values, marker='o')\n",
        "    ax[0].set_xlabel(\"Number of Clusters\")\n",
        "    ax[0].set_ylabel(\"Inertia\")\n",
        "    ax[0].set_title(\"Elbow Method\")\n",
        "\n",
        "    # Determine the best number of clusters using Silhouette Analysis\n",
        "    ax[1].plot(range(2, max_clusters + 1), silhouette_scores, marker='o')\n",
        "    ax[1].set_xlabel(\"Number of Clusters\")\n",
        "    ax[1].set_ylabel(\"Silhouette Score\")\n",
        "    ax[1].set_title(\"Silhouette Analysis\")\n",
        "\n",
        "    # st.pyplot(fig)\n",
        "\n",
        "    # Determine the best number of clusters through Trial and Error\n",
        "    best_clusters = st.slider(\"Puedes agrupar los artículos seleccionando el número deseado de agrupaciones\", 2, max_clusters, step=1)\n",
        "\n",
        "    # Apply K-means clustering with the selected number of clusters\n",
        "    kmeans = KMeans(n_clusters=best_clusters, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    # Get cluster labels\n",
        "    labels = kmeans.labels_\n",
        "\n",
        "    # Reduce dimensionality for visualization using PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X.toarray())\n",
        "\n",
        "    # Display the scatter plot of the clusters\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n",
        "    legend1 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
        "    ax.add_artist(legend1)\n",
        "    ax.set_xlabel(\"Principal Component 1\")\n",
        "    ax.set_ylabel(\"Principal Component 2\")\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Display the cluster labels with Abstract\n",
        "    cluster_data = pd.DataFrame(\n",
        "        {'Cluster': labels, 'Resúmenes': data['Abstract'], 'Título': data['Title'], })\n",
        "    st.write(\"Agrupaciones de artículos usando como insumo el resumen\")\n",
        "    st.write(cluster_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQBgHInnaj5z"
      },
      "source": [
        "# Keyword_Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNUPAE8sH2B_",
        "outputId": "8a12307f-9c03-4337-cb00-8669d5f26ce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting keyword_analysis.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile keyword_analysis.py\n",
        "\n",
        "import streamlit as st\n",
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "# Function to generate a word cloud\n",
        "def generate_wordcloud(text, title, st):\n",
        "    stopwords_list = stopwords.words('english')\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = ' '.join([word for word in word_tokens if word.lower() not in stopwords_list])\n",
        "\n",
        "    if filtered_text:\n",
        "        fig, ax = plt.subplots()\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(filtered_text)\n",
        "\n",
        "        ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "        ax.axis(\"off\")\n",
        "        ax.set_title(title)\n",
        "\n",
        "        st.pyplot(fig)\n",
        "\n",
        "\n",
        "def generate_keyword_analysis(data, st):\n",
        "    # Convert keyword values to strings\n",
        "    data['Author Keywords'] = data['Author Keywords'].astype(str)\n",
        "\n",
        "    # Split and select unique keywords\n",
        "    keywords = set(';'.join(data['Author Keywords']).split(';'))\n",
        "\n",
        "    # Select a keyword\n",
        "    keyword = st.selectbox(\"Palabras clave\", sorted(keywords))\n",
        "\n",
        "    # Filter the dataframe based on the keyword\n",
        "    if keyword:\n",
        "        filtered_data = data[data['Title'].str.contains(keyword, case=False) |\n",
        "                             data['Abstract'].str.contains(keyword, case=False) |\n",
        "                             data['Author Keywords'].str.contains(keyword, case=False)]\n",
        "\n",
        "        # Reset the index of the filtered data\n",
        "        filtered_data = filtered_data.reset_index(drop=True)\n",
        "        filtered_data = filtered_data.reset_index(drop=True)\n",
        "        filtered_data.index += 1\n",
        "\n",
        "        # Display word clouds\n",
        "        generate_wordcloud(' '.join(filtered_data['Title'].astype(str)), \"Nube de palabras por título\", st)\n",
        "        generate_wordcloud(' '.join(filtered_data['Abstract'].astype(str)), \"Nube de palabras por resumen\", st)\n",
        "        generate_wordcloud(' '.join(filtered_data['Author Keywords'].astype(str)),\n",
        "                           \"Nube de palabras por palabras clave\", st)\n",
        "\n",
        "        # Display the filtered dataframe\n",
        "        st.subheader(\"Artículos seleccionados\")\n",
        "        st.dataframe(filtered_data)\n",
        "    else:\n",
        "        st.info(\"Seleccionar una palabra clave.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do96nM4tapR1"
      },
      "source": [
        "# ODS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JjWcfWXBRNX",
        "outputId": "f22ccfda-52b7-4e49-c011-33cbfd01bbf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ODS.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ODS.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def ODS_analysis(data, st):\n",
        "    # Load the ODS dataframe\n",
        "    df_ods = pd.read_csv('output_OSD.csv')\n",
        "    df_ods = df_ods.drop(0)\n",
        "\n",
        "    # Load the data dataframe\n",
        "    df_all_data = data\n",
        "\n",
        "    # Create a selectbox to choose the \"Objetivo ODS\"\n",
        "    selected_objetivo_ods = st.selectbox(\"Selecciona un objetivo de desarrollo sostenible\", df_ods['Objetivo ODS'])\n",
        "\n",
        "    # Filter the data based on the selected \"Objetivo ODS\"\n",
        "    filtered_data = df_all_data[df_all_data['Title'].isin(\n",
        "        df_ods[df_ods['Objetivo ODS'] == selected_objetivo_ods][['Titulo 1', 'Titulo 2', 'Titulo 3']].values.flatten())]\n",
        "\n",
        "    # Display the filtered data\n",
        "    st.write(filtered_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPTNFBUBauAx"
      },
      "source": [
        "# H_Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy80gOgLPrFJ",
        "outputId": "1215bd7c-c803-472b-aca2-09d50b3b49ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting h_index.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile h_index.py\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "data = pd.read_csv(\"papersPreprocessedv3.csv\")\n",
        "data = data.fillna('')\n",
        "\n",
        "def get_authors():\n",
        "  authors_list = list(data['disambiguated_authors'].fillna(''))\n",
        "  authors_with_duplicates = []\n",
        "\n",
        "  for document in authors_list:\n",
        "    authors = document if document is not None else ''\n",
        "    authors_with_duplicates.extend(authors.split(';'))\n",
        "\n",
        "  list_without_duplicates = list(set(authors_with_duplicates))\n",
        "  list_without_duplicates.sort()\n",
        "\n",
        "  return list_without_duplicates\n",
        "\n",
        "\n",
        "def get_h_index(st):\n",
        "\n",
        "    authors_list = get_authors()\n",
        "\n",
        "    selected_node = st.selectbox(\"Seleccione el autor\", authors_list)\n",
        "\n",
        "    result = data.loc[data['disambiguated_authors'].str.contains(selected_node, regex=False), 'Cited by'].tolist()\n",
        "\n",
        "    num_articles = len(result)\n",
        "\n",
        "    range_h = range(1, num_articles + 1)\n",
        "\n",
        "    citas_acumuladas = [sum(result[:i]) for i in range_h]\n",
        "\n",
        "    # Gráfico del índice H\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(range_h, citas_acumuladas, marker='o')\n",
        "    ax.set(xlabel='Número de artículos', ylabel='Citas acumuladas', title='Índice H')\n",
        "    st.pyplot(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fonbb9s9ayA9"
      },
      "source": [
        "# Authors_top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZPEWOgoPvMD",
        "outputId": "c8934f7f-35d1-4765-a6ce-858e70a23873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting authors_top.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile authors_top.py\n",
        "import nltk\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def get_authors(data):\n",
        "    authors_list = data['Authors'].unique().tolist()\n",
        "\n",
        "    authors = []\n",
        "\n",
        "    for authors_values in authors_list:\n",
        "        authors.extend([item.strip() for item in authors_values.split(\"., \")])\n",
        "\n",
        "    palabras_conocidas = set(nltk.corpus.words.words())\n",
        "\n",
        "    # Identificar los nombres que no son palabras conocidas\n",
        "    nombres_sin_sentido = [nombre for nombre in authors if nombre.lower() not in palabras_conocidas]\n",
        "\n",
        "    return sorted(nombres_sin_sentido)\n",
        "\n",
        "\n",
        "def get_authors_top(data, st):\n",
        "    numbers_list = [5, 10, 15, 20]\n",
        "    selected_node = st.selectbox(\"Seleccione cuantos autores quiere ver\", numbers_list)\n",
        "\n",
        "    authors = get_authors(data)\n",
        "    counts = {}\n",
        "\n",
        "    for item in authors:\n",
        "        if item in counts:\n",
        "            counts[item] += 1\n",
        "        else:\n",
        "            counts[item] = 1\n",
        "\n",
        "    sorted_count = dict(sorted(counts.items(), key=lambda x: x[1], reverse=True))\n",
        "    top = dict(itertools.islice(sorted_count.items(), selected_node))\n",
        "\n",
        "    autores = list(top.keys())\n",
        "    conteos = list(top.values())\n",
        "\n",
        "    # Crear la figura y el gráfico de barras\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.bar(autores, conteos)\n",
        "\n",
        "    # Personalizar el gráfico\n",
        "    ax.set(xlabel='Autores', ylabel='Conteo', title='Top de Autores')\n",
        "\n",
        "    # Rotar las etiquetas del eje x para mayor legibilidad\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Mostrar el gráfico en Streamlit\n",
        "    st.pyplot(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI_EQcRHl6NM"
      },
      "source": [
        "# Countries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZznNPesqmBIt",
        "outputId": "71ed9989-4b2b-4ef7-9857-d4a12f89085b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting plot_countries.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile plot_countries.py\n",
        "\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "def countries_plot(data, st):\n",
        "    # Load world shapefile\n",
        "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
        "\n",
        "    # Convert country column to a list of lists\n",
        "    countries = data['country'].fillna('').apply(lambda x: list(set(str(x).split(';'))))\n",
        "\n",
        "    # Flatten the nested lists into a single list\n",
        "    flat_list = [item for sublist in countries for item in sublist]\n",
        "\n",
        "    # Count the occurrences of each country\n",
        "    country_counts = Counter(flat_list)\n",
        "\n",
        "    # Rename \"United States\" to \"United States of America\" in the country counts\n",
        "    if \"United States\" in country_counts:\n",
        "        country_counts[\"United States of America\"] = country_counts.pop(\"United States\")\n",
        "\n",
        "    # Merge country counts with world shapefile\n",
        "    world['Publication Count'] = world['name'].map(country_counts)\n",
        "\n",
        "    # Sort countries by count in descending order\n",
        "    sorted_countries = world.sort_values('Publication Count', ascending=False)\n",
        "\n",
        "    # Get top 10 countries\n",
        "    top_10_countries = sorted_countries.head(10)\n",
        "\n",
        "    # Plot the map\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    top_10_countries.plot(column='Publication Count', cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
        "\n",
        "    # Set plot title\n",
        "    ax.set_title('Distribución geográfica de las publicaciones')\n",
        "\n",
        "    # Remove axis ticks and labels\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    # Display the map in Streamlit\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Display the table\n",
        "    st.subheader('Top 10 de cantidad de publicaciones por países')\n",
        "    st.dataframe(top_10_countries[['name', 'Publication Count']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_sZLqQ-9bnr"
      },
      "source": [
        "# Cities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR1oVhxt9hSK",
        "outputId": "0b5d9057-5ff7-4a26-a4c9-1e20b34465ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting plot_cities.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile plot_cities.py\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import streamlit as st\n",
        "\n",
        "def plot_colombia_cities():\n",
        "\n",
        "    data = pd.read_csv('Collection_Correct_Name_Scopus.csv')\n",
        "\n",
        "    # Filter data for Colombia\n",
        "    colombia_data = data[data['affiliation_country'] == 'Colombia']\n",
        "\n",
        "    # Split affiliation cities and convert to a list of lists\n",
        "    cities = colombia_data['affiliation_city'].fillna('').apply(lambda x: list(set(str(x).split(';'))))\n",
        "\n",
        "    # Flatten the nested lists into a single list\n",
        "    flat_list = [item for sublist in cities for item in sublist]\n",
        "\n",
        "    # Count the occurrences of each city\n",
        "    city_counts = Counter(flat_list)\n",
        "\n",
        "    # Convert city_counts into a DataFrame\n",
        "    city_counts_df = pd.DataFrame.from_dict(city_counts, orient='index', columns=['Count'])\n",
        "\n",
        "    # Sort the DataFrame by city count in descending order\n",
        "    city_counts_df = city_counts_df.sort_values('Count', ascending=False)\n",
        "\n",
        "    # Reset the index of the DataFrame\n",
        "    city_counts_df = city_counts_df.reset_index().rename(columns={'index': 'City'})\n",
        "\n",
        "    # Display the table\n",
        "    st.subheader('Publicaciones por ciudades')\n",
        "    st.dataframe( city_counts_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpvTpIiujqxB"
      },
      "source": [
        "# Main APP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZiKTZhzyKst",
        "outputId": "7d9abb9b-a86d-4055-ff4a-c462a21ca132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "#Features\n",
        "from EDA import perform_eda\n",
        "from scopus_network import visualization_graph\n",
        "from afi1_network import visualization_graph_afi\n",
        "from cluster import detect_communities\n",
        "from keyword_analysis import generate_keyword_analysis\n",
        "from ODS import ODS_analysis\n",
        "from h_index import get_h_index\n",
        "from authors_top import get_authors_top\n",
        "from plot_countries import countries_plot\n",
        "from EDA_proy_inv import perform_eda_proy_inv\n",
        "from EDA_scopus import perform_eda_scopus\n",
        "from plot_cities import plot_colombia_cities\n",
        "\n",
        "    \n",
        "def main():\n",
        "    st.title(\"Redes de investigación de la PUJ\")\n",
        "    \n",
        "    # Read the CSV file\n",
        "    data = pd.read_csv('papersPreprocessed.csv')\n",
        "\n",
        "    # Sidebar selection\n",
        "    selection = st.sidebar.radio(\"Selecciona una opción\",\n",
        "     (\"Publicaciones Javerianas en Scopus\",\n",
        "      \"Publicaciones Javerianas en Web of Science y Scopus\",\n",
        "     # \"Exploración de Proyectos de Investigación\",\n",
        "     \"Redes de Investigadores\",\n",
        "     \"Red de la PUJ\",\n",
        "      \"Segmentación de artículos\",\n",
        "      \"Análisis de palabras claves\",\n",
        "      \"Relación de ODS con artículos\",\n",
        "      \"Índice H de artículos\",\n",
        "      \"Top de autores con más publicaciones\",\n",
        "      \"Visualización geográfica por países\", \n",
        "      \"Visualización geográfica por ciudades de Colombia\"))\n",
        "    if selection == \"Publicaciones Javerianas en Scopus\":\n",
        "        st.subheader(\"Conoce aspectos relevantes de los datos de investigadores de Scopus\")\n",
        "        perform_eda_scopus(st)    \n",
        "    elif selection == \"Publicaciones Javerianas en Web of Science y Scopus\":\n",
        "        st.subheader(\"Conoce aspectos relevantes de los datos de investigadores de Wos y Scopus\")\n",
        "        perform_eda(data, st)\n",
        "    elif selection == \"Exploración de Proyectos de Investigación\":\n",
        "        st.subheader(\"Conozcamos aspectos relevantes de la investigación\")\n",
        "        perform_eda_proy_inv(st)        \n",
        "    elif selection == \"Redes de Investigadores\":\n",
        "        st.subheader(\"Exploremos la red de autores y coautores de Scopus\")\n",
        "        visualization_graph(st)\n",
        "    elif selection == \"Red de la PUJ\":\n",
        "        st.subheader(\"Exploremos las afiliaciones de la PUJ\")        \n",
        "        visualization_graph_afi(st)        \n",
        "    elif selection == \"Segmentación de artículos\":\n",
        "        st.subheader(\"¿Quieres conocer grupos de artículos similares?\")\n",
        "        detect_communities(data, st)\n",
        "    elif selection == \"Análisis de palabras claves\":\n",
        "        st.subheader(\"Revisemos las palabras claves y sus artículos relacionados\")\n",
        "        generate_keyword_analysis(data, st)\n",
        "    elif selection == \"Relación de ODS con artículos\":\n",
        "        st.subheader(\"¿Cuál objetivo quieres revisar?\")\n",
        "        ODS_analysis(data, st)\n",
        "    elif selection == \"Índice H de artículos\":\n",
        "        st.subheader(\"Revisemos el índice de artículos\")\n",
        "        get_h_index(st)\n",
        "    elif selection == \"Top de autores con más publicaciones\":\n",
        "        st.subheader(\"Revisemos el top de autores con más publicaciones\")\n",
        "        get_authors_top(data, st)\n",
        "    elif selection == \"Visualización geográfica\":\n",
        "        st.subheader(\"Revisemos el tema a nivel mundial\")\n",
        "        countries_plot(data, st)\n",
        "    elif selection == \"Visualización geográfica por ciudades de Colombia\":\n",
        "        st.subheader(\"Revisemos el tema en Colombia\")\n",
        "        plot_colombia_cities()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFlvFzJ0jnws"
      },
      "source": [
        "# Sección nueva"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Ws-K0BZ4C9fH"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Frr7dCiDCSf",
        "outputId": "157fabb3-1f1a-4edc-dc91-3df05f5135a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mcli\u001b[0m npm v9.6.7 does not support Node.js v14.16.0. This version of npm supports the following node versions: `^14.17.0 || ^16.13.0 || >=18.0.0`. You can find the latest version at https://nodejs.org/.\n",
            "\u001b[0myour url is: https://stupid-adults-flash.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "tdIGXR6VaA9e"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}