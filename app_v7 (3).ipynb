{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "2YIAwM3laTnA",
    "Iajlld1V3EA6",
    "mX5sOu1sae1B",
    "nQBgHInnaj5z",
    "do96nM4tapR1",
    "aPTNFBUBauAx",
    "fonbb9s9ayA9",
    "yI_EQcRHl6NM"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Instalación de librerias"
   ],
   "metadata": {
    "id": "tdIGXR6VaA9e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2AENgY1SCnt0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1c21d47f-deeb-483f-c449-807d2b319e0c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[37;40mnpm\u001B[0m \u001B[0m\u001B[30;43mWARN\u001B[0m \u001B[0m\u001B[35mcli\u001B[0m npm v9.6.7 does not support Node.js v14.16.0. This version of npm supports the following node versions: `^14.17.0 || ^16.13.0 || >=18.0.0`. You can find the latest version at https://nodejs.org/.\n",
      "\u001B[0m\u001B[37;40mnpm\u001B[0m \u001B[0m\u001B[31;40mERR!\u001B[0m \u001B[0m\u001B[35mcode\u001B[0m EBADENGINE\n",
      "\u001B[0m\u001B[37;40mnpm\u001B[0m \u001B[0m\u001B[31;40mERR!\u001B[0m \u001B[0m\u001B[35mengine\u001B[0m Unsupported engine\n",
      "\u001B[0m\u001B[37;40mnpm\u001B[0m \u001B[0m\u001B[31;40mERR!\u001B[0m \u001B[0m\u001B[35mengine\u001B[0m Not compatible with your version of node/npm: npm@9.6.7\n",
      "\u001B[0m\u001B[37;40mnpm\u001B[0m \u001B[0m\u001B[31;40mERR!\u001B[0m \u001B[0m\u001B[35mnotsup\u001B[0m Not compatible with your version of node/npm: npm@9.6.7\n",
      "\u001B[0m\u001B[37;40mnpm\u001B[0m \u001B[0m\u001B[31;40mERR!\u001B[0m \u001B[0m\u001B[35mnotsup\u001B[0m Required: {\"node\":\"^14.17.0 || ^16.13.0 || >=18.0.0\"}\n",
      "\u001B[0m\u001B[37;40mnpm\u001B[0m \u001B[0m\u001B[31;40mERR!\u001B[0m \u001B[0m\u001B[35mnotsup\u001B[0m Actual:   {\"npm\":\"9.6.7\",\"node\":\"v14.16.0\"}\n",
      "\u001B[0m\n",
      "\u001B[37;40mnpm\u001B[0m \u001B[0m\u001B[31;40mERR!\u001B[0m\u001B[35m\u001B[0m A complete log of this run can be found in: /root/.npm/_logs/2023-05-20T23_38_32_794Z-debug-0.log\n",
      "\u001B[0m\u001B[37;40mnpm\u001B[0m \u001B[0m\u001B[30;43mWARN\u001B[0m \u001B[0m\u001B[35mcli\u001B[0m npm v9.6.7 does not support Node.js v14.16.0. This version of npm supports the following node versions: `^14.17.0 || ^16.13.0 || >=18.0.0`. You can find the latest version at https://nodejs.org/.\n",
      "\u001B[K\u001B[?25h\n",
      "up to date, audited 23 packages in 692ms\n",
      "\n",
      "3 packages are looking for funding\n",
      "  run `npm fund` for details\n",
      "\n",
      "found \u001B[32m\u001B[1m0\u001B[22m\u001B[39m vulnerabilities\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pyvis in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
      "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.8.2.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Collecting geopandas\n",
      "  Downloading geopandas-0.13.0-py3-none-any.whl (1.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m14.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis) (7.34.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.1.2)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.0.1)\n",
      "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.22.4)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
      "Collecting fiona>=1.8.19 (from geopandas)\n",
      "  Downloading Fiona-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.5/16.5 MB\u001B[0m \u001B[31m38.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas) (23.1)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from geopandas) (1.5.3)\n",
      "Collecting pyproj>=3.0.1 (from geopandas)\n",
      "  Downloading pyproj-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.7/7.7 MB\u001B[0m \u001B[31m64.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: shapely>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from geopandas) (2.0.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (23.1.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (2022.12.7)\n",
      "Collecting click-plugins>=1.0 (from fiona>=1.8.19->geopandas)\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting cligj>=0.5 (from fiona>=1.8.19->geopandas)\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (1.16.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (67.7.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.18.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (3.0.38)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (2.14.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9.6->pyvis) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas) (2022.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.6)\n",
      "Installing collected packages: pyproj, cligj, click-plugins, fiona, geopandas\n",
      "Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.9.4 geopandas-0.13.0 pyproj-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -q streamlit\n",
    "!npm install -g npm\n",
    "!npm install localtunnel\n",
    "!pip install pyvis wordcloud nltk geopandas\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "stop_words_english = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbFqU0gIUEtY",
    "outputId": "f35b5328-6c0b-485f-f14c-0c4ea12775a8"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EDA"
   ],
   "metadata": {
    "id": "2YIAwM3laTnA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "% % writefile EDA.py\n",
    "\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Perform exploratory data analysis (EDA)\n",
    "def perform_eda(data, st):\n",
    "    # Mostrar los datos\n",
    "    st.subheader(\"Datos Previos\")\n",
    "    st.dataframe(data.head())\n",
    "\n",
    "    # Número de registros\n",
    "    st.write(\"Número de observaciones:\", data.shape[0])\n",
    "    st.write(\"Número de variables:\", data.shape[1])\n",
    "\n",
    "    # Estadísticas resumidas\n",
    "    st.write(\"Resumen estadístico:\")\n",
    "    st.write(data.describe(include='all'))\n",
    "\n",
    "    # Valores faltantes\n",
    "    st.write(\"Valores faltantes:\")\n",
    "    conteo_valores_faltantes = data.isnull().sum()\n",
    "    porcentaje_valores_faltantes = (conteo_valores_faltantes / len(data)) * 100\n",
    "\n",
    "    valores_faltantes_df = pd.DataFrame({\n",
    "        \"Columna\": conteo_valores_faltantes.index,\n",
    "        \"Conteo de Faltantes\": conteo_valores_faltantes.values,\n",
    "        \"Porcentaje de Faltantes\": [f\"{porcentaje:.2f}%\" for porcentaje in porcentaje_valores_faltantes.values]\n",
    "    })\n",
    "    st.dataframe(valores_faltantes_df)\n",
    "\n",
    "    # Análisis adicional de EDA y visualizaciones\n",
    "    st.subheader(\"Análisis de algunas variables relevantes\")\n",
    "\n",
    "    ## Value counts para 'Patrocinadores'\n",
    "    #st.write(\"Top 10 de autores más frecuentes:\")\n",
    "    #top_fund = data['fund_sponsor'].value_counts().head(10)\n",
    "    #st.write(top_fund)\n",
    "\n",
    "    # Value counts para 'Authors'\n",
    "    st.write(\"Top 10 de autores más frecuentes:\")\n",
    "    top_autores = data['Authors'].value_counts().head(10)\n",
    "    st.write(top_autores)\n",
    "\n",
    "    # Value counts para 'Affiliations'\n",
    "    st.write(\"Top 10 de Afiliaciones más frecuentes:\")\n",
    "    top_Affiliations = data['Affiliations'].value_counts().head(10)\n",
    "    st.write(top_Affiliations)\n",
    "\n",
    "    # Value counts para 'Year'\n",
    "    st.write(\"Ranking de publicaciones por año:\")\n",
    "    value_counts_year = data['Year'].value_counts()\n",
    "    st.write(value_counts_year)\n",
    "\n",
    "    # Value counts para 'country'\n",
    "    st.write(\"Top 10 de ciudades más frecuentes:\")\n",
    "    top_country = data['country'].value_counts().head(10)\n",
    "    st.write(top_country)\n",
    "\n",
    "    # Value counts para 'institution'\n",
    "    st.write(\"Top 10 de instituciones más frecuentes:\")\n",
    "    top_institution = data['institution'].value_counts().head(10)\n",
    "    st.write(top_institution)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eeyI8HgZqbwU",
    "outputId": "79274263-cda5-4867-f452-465a61ef6664"
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting EDA.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EDA Proyectos de investigación"
   ],
   "metadata": {
    "id": "Iajlld1V3EA6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "% % writefile EDA_proy_inv.py\n",
    "\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Perform exploratory data analysis (EDA)\n",
    "def perform_eda_proy_inv(st):\n",
    "    data = pd.read_csv('projects.csv')\n",
    "    # Mostrar los datos\n",
    "    st.subheader(\"Datos Previos\")\n",
    "    st.dataframe(data.head())\n",
    "\n",
    "    # Número de registros\n",
    "    st.write(\"Número de observaciones:\", data.shape[0])\n",
    "    st.write(\"Número de variables:\", data.shape[1])\n",
    "\n",
    "    # Estadísticas resumidas\n",
    "    st.write(\"Resumen estadístico:\")\n",
    "    st.write(data.describe(include='all'))\n",
    "\n",
    "    # Valores faltantes\n",
    "    st.write(\"Valores faltantes:\")\n",
    "    conteo_valores_faltantes = data.isnull().sum()\n",
    "    porcentaje_valores_faltantes = (conteo_valores_faltantes / len(data)) * 100\n",
    "\n",
    "    valores_faltantes_df = pd.DataFrame({\n",
    "        \"Columna\": conteo_valores_faltantes.index,\n",
    "        \"Conteo de Faltantes\": conteo_valores_faltantes.values,\n",
    "        \"Porcentaje de Faltantes\": [f\"{porcentaje:.2f}%\" for porcentaje in porcentaje_valores_faltantes.values]\n",
    "    })\n",
    "    st.dataframe(valores_faltantes_df)\n",
    "\n",
    "    # Análisis adicional de EDA y visualizaciones\n",
    "    st.subheader(\"Análisis de algunas variables relevantes\")\n",
    "\n",
    "    # Value counts para 'Authors'\n",
    "    st.write(\"Top 10 de autores más frecuentes:\")\n",
    "    top_autores = data['autores'].value_counts().head(10)\n",
    "    st.write(top_autores)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1C4TOOi_3Hqv",
    "outputId": "a5620760-bf07-45ca-d39b-6dd29f9553f5"
   },
   "execution_count": 43,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting EDA_proy_inv.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EDA WoS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "% % writefile EDA_wos.py\n",
    "\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Perform exploratory data analysis (EDA)\n",
    "def perform_eda_wos(st):\n",
    "    data = pd.read_csv('wos.csv')\n",
    "    # Mostrar los datos\n",
    "    st.subheader(\"Datos Previos\")\n",
    "    st.dataframe(data.head())\n",
    "\n",
    "    # Número de registros\n",
    "    st.write(\"Número de observaciones:\", data.shape[0])\n",
    "    st.write(\"Número de variables:\", data.shape[1])\n",
    "\n",
    "    # Estadísticas resumidas\n",
    "    st.write(\"Resumen estadístico:\")\n",
    "    st.write(data.describe(include='all'))\n",
    "\n",
    "    # Valores faltantes\n",
    "    st.write(\"Valores faltantes:\")\n",
    "    conteo_valores_faltantes = data.isnull().sum()\n",
    "    porcentaje_valores_faltantes = (conteo_valores_faltantes / len(data)) * 100\n",
    "\n",
    "    valores_faltantes_df = pd.DataFrame({\n",
    "        \"Columna\": conteo_valores_faltantes.index,\n",
    "        \"Conteo de Faltantes\": conteo_valores_faltantes.values,\n",
    "        \"Porcentaje de Faltantes\": [f\"{porcentaje:.2f}%\" for porcentaje in porcentaje_valores_faltantes.values]\n",
    "    })\n",
    "    st.dataframe(valores_faltantes_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scopus Network"
   ],
   "metadata": {
    "id": "_aEL2I2_aY7G"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "% % writefile scopus_network.py\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "\n",
    "\n",
    "# Function to load the nodes and edges CSV files\n",
    "def load_data_network():\n",
    "    nodes_df = pd.read_csv('nodes.csv')\n",
    "    edges_df = pd.read_csv('edges.csv')\n",
    "    return nodes_df, edges_df\n",
    "\n",
    "\n",
    "# Function to create the network graph using NetworkX\n",
    "def create_graph(nodes_df, edges_df):\n",
    "    G = nx.Graph()\n",
    "    for _, row in nodes_df.iterrows():\n",
    "        G.add_node(row['Node'], ID=row['ID'])\n",
    "    for _, row in edges_df.iterrows():\n",
    "        G.add_edge(row['Source'], row['Target'])\n",
    "    return G\n",
    "\n",
    "\n",
    "# Function to get the first and second tier nodes for a given node\n",
    "def get_tiers(G, node):\n",
    "    first_tier = list(G.neighbors(node))\n",
    "    second_tier = []\n",
    "    for neighbor in first_tier:\n",
    "        second_tier.extend(list(G.neighbors(neighbor)))\n",
    "    second_tier = list(set(second_tier))  # Remove duplicates\n",
    "    return first_tier, second_tier\n",
    "\n",
    "\n",
    "# Create graph visualization based on selected attribute\n",
    "def visualization_graph(st):\n",
    "    # Load the data\n",
    "    nodes_df, edges_df = load_data_network()\n",
    "\n",
    "    # Create the graph\n",
    "    G = create_graph(nodes_df, edges_df)\n",
    "\n",
    "    # Dropdown to select a node\n",
    "    selected_node = st.selectbox(\"Seleccione el autor\", nodes_df['Node'])\n",
    "\n",
    "    # Get first and second tier nodes\n",
    "    first_tier_nodes, second_tier_nodes = get_tiers(G, selected_node)\n",
    "\n",
    "    # Subgraph containing the selected node and its first and second tier nodes\n",
    "    subgraph = G.subgraph([selected_node] + first_tier_nodes + second_tier_nodes)\n",
    "\n",
    "    # Visualization using pyvis\n",
    "    network = Network(width=\"100%\", height=\"800px\", notebook=True)\n",
    "    network.from_nx(subgraph)\n",
    "    network.show(\"network.html\")\n",
    "\n",
    "    # Display the network visualization\n",
    "    st.components.v1.html(open(\"network.html\", 'r', encoding='utf-8').read(), width=1000, height=600)\n",
    "\n",
    "    # Display first and second tier nodes\n",
    "    st.write(\"Coautores del investigador:\")\n",
    "    st.write(first_tier_nodes)\n",
    "    st.write(\"Coautores de los coautores:\")\n",
    "    st.write(second_tier_nodes)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5jffL7dtd61",
    "outputId": "3a8489a7-9807-4f56-a6fd-f8ec2c589fdc"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting scopus_network.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custers"
   ],
   "metadata": {
    "id": "mX5sOu1sae1B"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "% % writefile cluster.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "# Detect communities in the graph\n",
    "def detect_communities(data, st):\n",
    "    # Load the data\n",
    "    df = data\n",
    "\n",
    "    # Select relevant columns for clustering\n",
    "    data = df[['Abstract', 'Author Keywords', 'Index Keywords', 'Authors', 'Title']].dropna()\n",
    "\n",
    "    # Splitting the words by semicolon\n",
    "    data['Author Keywords'] = df['Author Keywords'].str.split(';')\n",
    "\n",
    "    # Flattening the list of words and applying filters\n",
    "    data['Author Keywords'] = data['Author Keywords'].apply(\n",
    "        lambda x: [re.sub(r'[^\\w\\s]', '', word.strip()) for word in x if len(word.strip()) > 4])\n",
    "\n",
    "    # Joining the words back into a single string\n",
    "    data['Author Keywords'] = data['Author Keywords'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(data['Author Keywords'])\n",
    "\n",
    "    # Elbow Method\n",
    "    inertia_values = []\n",
    "    silhouette_scores = []\n",
    "    max_clusters = 10\n",
    "\n",
    "    for n_clusters in range(2, max_clusters + 1):\n",
    "        # Apply K-means clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        kmeans.fit(X)\n",
    "\n",
    "        # Get cluster labels\n",
    "        labels = kmeans.labels_\n",
    "        inertia = kmeans.inertia_\n",
    "        silhouette_avg = silhouette_score(X, labels)\n",
    "\n",
    "        inertia_values.append(inertia)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Determine the best number of clusters using the Elbow Method\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].plot(range(2, max_clusters + 1), inertia_values, marker='o')\n",
    "    ax[0].set_xlabel(\"Number of Clusters\")\n",
    "    ax[0].set_ylabel(\"Inertia\")\n",
    "    ax[0].set_title(\"Elbow Method\")\n",
    "\n",
    "    # Determine the best number of clusters using Silhouette Analysis\n",
    "    ax[1].plot(range(2, max_clusters + 1), silhouette_scores, marker='o')\n",
    "    ax[1].set_xlabel(\"Number of Clusters\")\n",
    "    ax[1].set_ylabel(\"Silhouette Score\")\n",
    "    ax[1].set_title(\"Silhouette Analysis\")\n",
    "\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    # Determine the best number of clusters through Trial and Error\n",
    "    best_clusters = st.slider(\"Seleccione el número de clusters\", 2, max_clusters, step=1)\n",
    "\n",
    "    # Apply K-means clustering with the selected number of clusters\n",
    "    kmeans = KMeans(n_clusters=best_clusters, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    # Get cluster labels\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Reduce dimensionality for visualization using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X.toarray())\n",
    "\n",
    "    # Streamlit app\n",
    "    st.subheader(\" Grupos de artículos relacionados por palabras claves\")\n",
    "\n",
    "    # Display the scatter plot of the clusters\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n",
    "    legend1 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "    ax.add_artist(legend1)\n",
    "    ax.set_xlabel(\"Principal Component 1\")\n",
    "    ax.set_ylabel(\"Principal Component 2\")\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    # Display the cluster labels with author names\n",
    "    cluster_data = pd.DataFrame(\n",
    "        {'Cluster': labels, 'Palabras claves': data['Author Keywords'], 'Título': data['Title'], })\n",
    "    st.write(\"Agrupaciones de artículos por palabras claves\")\n",
    "    st.write(cluster_data)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16A1JMPvv27K",
    "outputId": "7fbe4bed-e329-4526-a37c-f706414760ce"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing cluster.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Keyword_Analysis"
   ],
   "metadata": {
    "id": "nQBgHInnaj5z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "% % writefile keyword_analysis.py\n",
    "\n",
    "import streamlit as st\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Function to generate a word cloud\n",
    "def generate_wordcloud(text, title, st):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = ' '.join([word for word in word_tokens if word.lower() not in stopwords_list])\n",
    "\n",
    "    if filtered_text:\n",
    "        fig, ax = plt.subplots()\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(filtered_text)\n",
    "\n",
    "        ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(title)\n",
    "\n",
    "        st.pyplot(fig)\n",
    "\n",
    "\n",
    "def generate_keyword_analysis(data, st):\n",
    "    # Convert keyword values to strings\n",
    "    data['Author Keywords'] = data['Author Keywords'].astype(str)\n",
    "\n",
    "    # Split and select unique keywords\n",
    "    keywords = set(';'.join(data['Author Keywords']).split(';'))\n",
    "\n",
    "    # Select a keyword\n",
    "    keyword = st.selectbox(\"Palabras clave\", sorted(keywords))\n",
    "\n",
    "    # Filter the dataframe based on the keyword\n",
    "    if keyword:\n",
    "        filtered_data = data[data['Title'].str.contains(keyword, case=False) |\n",
    "                             data['Abstract'].str.contains(keyword, case=False) |\n",
    "                             data['Author Keywords'].str.contains(keyword, case=False)]\n",
    "\n",
    "        # Reset the index of the filtered data\n",
    "        filtered_data = filtered_data.reset_index(drop=True)\n",
    "        filtered_data = filtered_data.reset_index(drop=True)\n",
    "        filtered_data.index += 1\n",
    "\n",
    "        # Display word clouds\n",
    "        generate_wordcloud(' '.join(filtered_data['Title'].astype(str)), \"Nube de palabras por título\", st)\n",
    "        generate_wordcloud(' '.join(filtered_data['Abstract'].astype(str)), \"Nube de palabras por resumen\", st)\n",
    "        generate_wordcloud(' '.join(filtered_data['Author Keywords'].astype(str)),\n",
    "                           \"Nube de palabras por palabras clave\", st)\n",
    "\n",
    "        # Display the filtered dataframe\n",
    "        st.subheader(\"Artículos seleccionados\")\n",
    "        st.dataframe(filtered_data)\n",
    "    else:\n",
    "        st.info(\"Seleccionar una palabra clave.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rNUPAE8sH2B_",
    "outputId": "a9560aad-4068-4c51-98c5-da625fbb3468"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing keyword_analysis.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ODS\n"
   ],
   "metadata": {
    "id": "do96nM4tapR1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "% % writefile ODS.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def ODS_analysis(data, st):\n",
    "    # Load the ODS dataframe\n",
    "    df_ods = pd.read_csv('output_OSD.csv')\n",
    "    df_ods = df_ods.drop(0)\n",
    "\n",
    "    # Load the data dataframe\n",
    "    df_all_data = data\n",
    "\n",
    "    # Create a selectbox to choose the \"Objetivo ODS\"\n",
    "    selected_objetivo_ods = st.selectbox(\"Selecciona un objetivo de desarrollo sostenible\", df_ods['Objetivo ODS'])\n",
    "\n",
    "    # Filter the data based on the selected \"Objetivo ODS\"\n",
    "    filtered_data = df_all_data[df_all_data['Title'].isin(\n",
    "        df_ods[df_ods['Objetivo ODS'] == selected_objetivo_ods][['Titulo 1', 'Titulo 2', 'Titulo 3']].values.flatten())]\n",
    "\n",
    "    # Display the filtered data\n",
    "    st.write(filtered_data)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JjWcfWXBRNX",
    "outputId": "1fdba35f-5137-497f-9cd9-3eaf84d92079"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing ODS.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# H_Index"
   ],
   "metadata": {
    "id": "aPTNFBUBauAx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "% % writefile h_index.py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "\n",
    "def calculate_h_index(cited_by):\n",
    "    ordered = sorted(cited_by, reverse=True)\n",
    "    h_index = 0\n",
    "    for i, cita in enumerate(ordered):\n",
    "        if cita >= i + 1:\n",
    "            h_index = i + 1\n",
    "        else:\n",
    "            break\n",
    "    return h_index\n",
    "\n",
    "\n",
    "def get_authors(data):\n",
    "    authors_list = data['Authors'].unique().tolist()\n",
    "\n",
    "    authors = []\n",
    "\n",
    "    for authors_values in authors_list:\n",
    "        authors.extend([item.strip() for item in authors_values.split(\"., \")])\n",
    "\n",
    "    palabras_conocidas = set(nltk.corpus.words.words())\n",
    "\n",
    "    # Identificar los nombres que no son palabras conocidas\n",
    "    nombres_sin_sentido = [nombre for nombre in authors if nombre.lower() not in palabras_conocidas]\n",
    "\n",
    "    return sorted(set(nombres_sin_sentido))\n",
    "\n",
    "\n",
    "def get_h_index(data, st):\n",
    "    authors_list = get_authors(data)\n",
    "\n",
    "    selected_node = st.selectbox(\"Seleccione el autor\", authors_list)\n",
    "\n",
    "    result = data.loc[data['authorFull'].str.contains(selected_node, regex=False), 'Cited by'].tolist()\n",
    "\n",
    "    num_articles = len(result)\n",
    "\n",
    "    range_h = range(1, num_articles + 1)\n",
    "\n",
    "    citas_acumuladas = [sum(result[:i]) for i in range_h]\n",
    "\n",
    "    h_index = calculate_h_index(citas_acumuladas)\n",
    "\n",
    "    # Gráfico del índice H\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range_h, citas_acumuladas, marker='o')\n",
    "    ax.set(xlabel='Número de artículos', ylabel='Citas acumuladas', title='Índice H')\n",
    "    st.pyplot(fig)\n",
    "    st.write(f\"El índice H para el autor {selected_node} es {h_index}\")\n",
    "    st.write(\"El índice H se define como el valor más alto de H tal que el autor ha publicado al menos H artículos que han recibido al menos H citas cada uno.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yy80gOgLPrFJ",
    "outputId": "2bdcfcaf-edd9-40a4-883e-3401bcdfcead"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing h_index.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Authors_top"
   ],
   "metadata": {
    "id": "fonbb9s9ayA9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "% % writefile authors_top.py\n",
    "import nltk\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_authors(data):\n",
    "    authors_list = data['Authors'].unique().tolist()\n",
    "\n",
    "    authors = []\n",
    "\n",
    "    for authors_values in authors_list:\n",
    "        authors.extend([item.strip() for item in authors_values.split(\"., \")])\n",
    "\n",
    "    palabras_conocidas = set(nltk.corpus.words.words())\n",
    "\n",
    "    # Identificar los nombres que no son palabras conocidas\n",
    "    nombres_sin_sentido = [nombre for nombre in authors if nombre.lower() not in palabras_conocidas]\n",
    "\n",
    "    return sorted(nombres_sin_sentido)\n",
    "\n",
    "\n",
    "def get_authors_top(data, st):\n",
    "    numbers_list = [5, 10, 15, 20]\n",
    "    selected_node = st.selectbox(\"Seleccione cuantos autores quiere ver\", numbers_list)\n",
    "\n",
    "    authors = get_authors(data)\n",
    "    counts = {}\n",
    "\n",
    "    for item in authors:\n",
    "        if item in counts:\n",
    "            counts[item] += 1\n",
    "        else:\n",
    "            counts[item] = 1\n",
    "\n",
    "    sorted_count = dict(sorted(counts.items(), key=lambda x: x[1], reverse=True))\n",
    "    top = dict(itertools.islice(sorted_count.items(), selected_node))\n",
    "\n",
    "    autores = list(top.keys())\n",
    "    conteos = list(top.values())\n",
    "\n",
    "    # Crear la figura y el gráfico de barras\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(autores, conteos)\n",
    "\n",
    "    # Personalizar el gráfico\n",
    "    ax.set(xlabel='Autores', ylabel='Conteo', title='Top de Autores')\n",
    "\n",
    "    # Rotar las etiquetas del eje x para mayor legibilidad\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Mostrar el gráfico en Streamlit\n",
    "    st.pyplot(fig)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZPEWOgoPvMD",
    "outputId": "b8cb3a38-9cba-4fd4-c123-cad38de0cc51"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing authors_top.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Countries"
   ],
   "metadata": {
    "id": "yI_EQcRHl6NM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "% % writefile plot_countries.py\n",
    "\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def countries_plot(data, st):\n",
    "    # Load world shapefile\n",
    "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "    countries = data['country'].str.split(';')\n",
    "    country_counts = {}\n",
    "    for country_list in countries:\n",
    "        if isinstance(country_list, list):\n",
    "            for country in country_list:\n",
    "                if pd.notnull(country):\n",
    "                    country = country.strip()\n",
    "                    if country in country_counts:\n",
    "                        country_counts[country] += 1\n",
    "                    else:\n",
    "                        country_counts[country] = 1\n",
    "\n",
    "    # Merge country counts with world shapefile\n",
    "    world['Publication Count'] = world['name'].map(country_counts)\n",
    "\n",
    "    # Sort countries by count in descending order\n",
    "    sorted_countries = world.sort_values('Publication Count', ascending=False)\n",
    "\n",
    "    # Get top 10 countries\n",
    "    top_10_countries = sorted_countries.head(10)\n",
    "\n",
    "    # Plot the map\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    top_10_countries.plot(column='Publication Count', cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
    "\n",
    "    # Set plot title\n",
    "    ax.set_title('Distribución geográfica de las publicaciones')\n",
    "\n",
    "    # Remove axis ticks and labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Display the map in Streamlit\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    # Display the table\n",
    "    st.subheader('Top 10 de cantidad de publicaciones por paises')\n",
    "    st.dataframe(top_10_countries[['name', 'Publication Count']])\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZznNPesqmBIt",
    "outputId": "a5386700-b39d-472a-f5b3-2f1774b493f0"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing plot_countries.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main APP"
   ],
   "metadata": {
    "id": "BpvTpIiujqxB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "% % writefile app.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "#Features\n",
    "from EDA import perform_eda\n",
    "from scopus_network import visualization_graph\n",
    "from cluster import detect_communities\n",
    "from keyword_analysis import generate_keyword_analysis\n",
    "from ODS import ODS_analysis\n",
    "from h_index import get_h_index\n",
    "from authors_top import get_authors_top\n",
    "from plot_countries import countries_plot\n",
    "from EDA_proy_inv import perform_eda_proy_inv\n",
    "\n",
    "\n",
    "def main():\n",
    "    st.title(\"Redes de investigación de la PUJ\")\n",
    "\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv('papersPreprocessed.csv')\n",
    "\n",
    "    # Sidebar selection\n",
    "    selection = st.sidebar.radio(\"Selecciona una opción\",\n",
    "                                 (\"Exploración de datos de WoS y Scopus\",\n",
    "                                  \"Exploración de Proyectos de Investigación\",\n",
    "                                  \"Redes de Investigadores\",\n",
    "                                  \"Segmentación de artículos\",\n",
    "                                  \"Análisis de palabras claves\",\n",
    "                                  \"Relación de ODS con artículos\",\n",
    "                                  \"Indice H de citaciones\",\n",
    "                                  \"Top de autores con más publicaciones\",\n",
    "                                  \"Visualización geográfica\"))\n",
    "\n",
    "    if selection == \"Exploración de datos de WoS y Scopus\":\n",
    "        st.subheader(\"Conoce aspectos relevantes de los datos de investigadares\")\n",
    "        perform_eda(data, st)\n",
    "    elif selection == \"Redes de Investigadores\":\n",
    "        st.subheader(\"Exploremos la red de autores y coautores de Scopus\")\n",
    "        visualization_graph(st)\n",
    "    elif selection == \"Segmentación de artículos\":\n",
    "        st.subheader(\"Definamos algunos grupos de artículos\")\n",
    "        detect_communities(data, st)\n",
    "    elif selection == \"Análisis de palabras claves\":\n",
    "        st.subheader(\"Revisemos las palabras claves y sus artículos relacionados\")\n",
    "        generate_keyword_analysis(data, st)\n",
    "    elif selection == \"Relación de ODS con artículos\":\n",
    "        st.subheader(\"¿Cuál objetivo quieres revisar?\")\n",
    "        ODS_analysis(data, st)\n",
    "    elif selection == \"Indice H de citaciones\":\n",
    "        st.subheader(\"Revisemos el índice de citaciones\")\n",
    "        get_h_index(data, st)\n",
    "    elif selection == \"Top de autores con mas publicaciones\":\n",
    "        st.subheader(\"Revisemos el top de autores con más publicaciones\")\n",
    "        get_authors_top(data, st)\n",
    "    elif selection == \"Visualización geográfica\":\n",
    "        st.subheader(\"Revisemos el tema a nivel mundial\")\n",
    "        countries_plot(data, st)\n",
    "    elif selection == \"Exploración de Proyectos de Investigación\":\n",
    "        st.subheader(\"Revisemos el tema a nivel mundial\")\n",
    "        perform_eda_proy_inv(st)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sZiKTZhzyKst",
    "outputId": "c6326466-a535-4d61-8956-ebc3ed8af448"
   },
   "execution_count": 42,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sección nueva"
   ],
   "metadata": {
    "id": "WFlvFzJ0jnws"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!streamlit run app.py & > / content / logs.txt &"
   ],
   "metadata": {
    "id": "Ws-K0BZ4C9fH"
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!npx localtunnel --port 8501"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Frr7dCiDCSf",
    "outputId": "c1b47906-de29-46c0-b4f0-9152ff348802"
   },
   "execution_count": 45,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[37;40mnpm\u001B[0m \u001B[0m\u001B[30;43mWARN\u001B[0m \u001B[0m\u001B[35mcli\u001B[0m npm v9.6.7 does not support Node.js v14.16.0. This version of npm supports the following node versions: `^14.17.0 || ^16.13.0 || >=18.0.0`. You can find the latest version at https://nodejs.org/.\n",
      "\u001B[0myour url is: https://pink-wings-double.loca.lt\n",
      "^C\n"
     ]
    }
   ]
  }
 ]
}