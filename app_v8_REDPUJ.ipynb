{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2YIAwM3laTnA",
        "Iajlld1V3EA6",
        "mX5sOu1sae1B",
        "nQBgHInnaj5z",
        "do96nM4tapR1",
        "aPTNFBUBauAx",
        "fonbb9s9ayA9",
        "yI_EQcRHl6NM",
        "cu8fIfwYW8sr"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instalación de librerias"
      ],
      "metadata": {
        "id": "tdIGXR6VaA9e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2AENgY1SCnt0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b61779c-fad2-43cc-8480-1ae2fd4adf4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K\u001b[?25h/tools/node/bin/npm -> /tools/node/lib/node_modules/npm/bin/npm-cli.js\n",
            "/tools/node/bin/npx -> /tools/node/lib/node_modules/npm/bin/npx-cli.js\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m Unsupported engine for npm@9.6.7: wanted: {\"node\":\"^14.17.0 || ^16.13.0 || >=18.0.0\"} (current: {\"node\":\"14.16.0\",\"npm\":\"6.14.8\"})\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m Not compatible with your version of node/npm: npm@9.6.7\n",
            "\u001b[0m\n",
            "+ npm@9.6.7\n",
            "added 120 packages from 31 contributors, removed 291 packages and updated 143 packages in 7.382s\n",
            "\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mcli\u001b[0m npm v9.6.7 does not support Node.js v14.16.0. This version of npm supports the following node versions: `^14.17.0 || ^16.13.0 || >=18.0.0`. You can find the latest version at https://nodejs.org/.\n",
            "\u001b[K\u001b[?25h\n",
            "added 22 packages in 1s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyvis\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.8.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting geopandas\n",
            "  Downloading geopandas-0.13.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis) (7.34.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.1.2)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.0.1)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.1)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (8.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Collecting fiona>=1.8.19 (from geopandas)\n",
            "  Downloading Fiona-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas) (23.1)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from geopandas) (1.5.3)\n",
            "Collecting pyproj>=3.0.1 (from geopandas)\n",
            "  Downloading pyproj-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: shapely>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from geopandas) (2.0.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (23.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (2022.12.7)\n",
            "Collecting click-plugins>=1.0 (from fiona>=1.8.19->geopandas)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting cligj>=0.5 (from fiona>=1.8.19->geopandas)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (1.16.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis)\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9.6->pyvis) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas) (2022.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.0.9)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.6)\n",
            "Installing collected packages: pyproj, jedi, cligj, click-plugins, fiona, pyvis, geopandas\n",
            "Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.9.4 geopandas-0.13.0 jedi-0.18.2 pyproj-3.5.0 pyvis-0.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit\n",
        "!npm install -g npm\n",
        "!npm install localtunnel\n",
        "!pip install pyvis wordcloud nltk geopandas gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "stop_words_english = nltk.corpus.stopwords.words('english')\n",
        "stemmer = SnowballStemmer('english')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbFqU0gIUEtY",
        "outputId": "633e3a2e-5ec0-459f-d267-0f165a48e0df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "2YIAwM3laTnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile EDA.py\n",
        "\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Perform exploratory data analysis (EDA)\n",
        "def perform_eda(data, st):\n",
        "\n",
        "    # Mostrar los datos\n",
        "    st.subheader(\"Datos Previos\")\n",
        "    st.dataframe(data.head())\n",
        "\n",
        "    # Número de registros\n",
        "    st.write(\"Número de observaciones:\", data.shape[0])\n",
        "    st.write(\"Número de variables:\", data.shape[1])\n",
        "\n",
        "    # Estadísticas resumidas\n",
        "    st.write(\"Resumen estadístico:\")\n",
        "    st.write(data.describe(include='all'))\n",
        "\n",
        "    # Valores faltantes\n",
        "    st.write(\"Valores faltantes:\")\n",
        "    conteo_valores_faltantes = data.isnull().sum()\n",
        "    porcentaje_valores_faltantes = (conteo_valores_faltantes / len(data)) * 100\n",
        "\n",
        "    valores_faltantes_df = pd.DataFrame({\n",
        "        \"Columna\": conteo_valores_faltantes.index,\n",
        "        \"Conteo de Faltantes\": conteo_valores_faltantes.values,\n",
        "        \"Porcentaje de Faltantes\": [f\"{porcentaje:.2f}%\" for porcentaje in porcentaje_valores_faltantes.values]\n",
        "    })\n",
        "    st.dataframe(valores_faltantes_df)\n",
        "\n",
        "    # Análisis adicional de EDA y visualizaciones\n",
        "    st.subheader(\"Análisis de algunas variables relevantes\")\n",
        "\n",
        "    ## Value counts para 'Patrocinadores'\n",
        "    #st.write(\"Top 10 de autores más frecuentes:\")\n",
        "    #top_fund = data['fund_sponsor'].value_counts().head(10)\n",
        "    #st.write(top_fund)\n",
        "\n",
        "    # Value counts para 'Authors'\n",
        "    st.write(\"Top 10 de autores más frecuentes:\")\n",
        "    top_autores = data['Authors'].value_counts().head(10)\n",
        "    st.write(top_autores)\n",
        "\n",
        "    # Value counts para 'Affiliations'\n",
        "    st.write(\"Top 10 de Afiliaciones más frecuentes:\")\n",
        "    top_Affiliations = data['Affiliations'].value_counts().head(10)\n",
        "    st.write(top_Affiliations)\n",
        "\n",
        "    # Value counts para 'Year'\n",
        "    st.write(\"Ranking de publicaciones por año:\")\n",
        "    value_counts_year = data['Year'].value_counts()\n",
        "    st.write(value_counts_year)\n",
        "\n",
        "    # Value counts para 'country'\n",
        "    st.write(\"Top 10 de ciudades más frecuentes:\")\n",
        "    top_country = data['country'].value_counts().head(10)\n",
        "    st.write(top_country)\n",
        "\n",
        "    # Value counts para 'institution'\n",
        "    st.write(\"Top 10 de instituciones más frecuentes:\")\n",
        "    top_institution = data['institution'].value_counts().head(10)\n",
        "    st.write(top_institution)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeyI8HgZqbwU",
        "outputId": "257d8adb-1d70-4a50-8927-cc38f486f5c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing EDA.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA Proyectos de investigación"
      ],
      "metadata": {
        "id": "Iajlld1V3EA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile EDA_proy_inv.py\n",
        "\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Perform exploratory data analysis (EDA)\n",
        "def perform_eda_proy_inv(st):\n",
        "\n",
        "    data = pd.read_csv('projects.csv')\n",
        "    # Mostrar los datos\n",
        "    st.subheader(\"Datos Previos\")\n",
        "    st.dataframe(data.head())\n",
        "\n",
        "    # Número de registros\n",
        "    st.write(\"Número de observaciones:\", data.shape[0])\n",
        "    st.write(\"Número de variables:\", data.shape[1])\n",
        "\n",
        "    # Estadísticas resumidas\n",
        "    st.write(\"Resumen estadístico:\")\n",
        "    st.write(data.describe(include='all'))\n",
        "\n",
        "    # Valores faltantes\n",
        "    st.write(\"Valores faltantes:\")\n",
        "    conteo_valores_faltantes = data.isnull().sum()\n",
        "    porcentaje_valores_faltantes = (conteo_valores_faltantes / len(data)) * 100\n",
        "\n",
        "    valores_faltantes_df = pd.DataFrame({\n",
        "        \"Columna\": conteo_valores_faltantes.index,\n",
        "        \"Conteo de Faltantes\": conteo_valores_faltantes.values,\n",
        "        \"Porcentaje de Faltantes\": [f\"{porcentaje:.2f}%\" for porcentaje in porcentaje_valores_faltantes.values]\n",
        "    })\n",
        "    st.dataframe(valores_faltantes_df)\n",
        "\n",
        "    # Análisis adicional de EDA y visualizaciones\n",
        "    st.subheader(\"Análisis de algunas variables relevantes\")\n",
        "\n",
        "    # Value counts para 'Authors'\n",
        "    st.write(\"Top 10 de autores más frecuentes:\")\n",
        "    top_autores = data['autores'].value_counts().head(10)\n",
        "    st.write(top_autores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C4TOOi_3Hqv",
        "outputId": "104b6707-ab0e-4242-c965-e23f8164903a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing EDA_proy_inv.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA Scopus"
      ],
      "metadata": {
        "id": "HYdNtkqS8Whp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile EDA_scopus.py\n",
        "\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Perform exploratory data analysis (EDA)\n",
        "def perform_eda_scopus(st):\n",
        "\n",
        "    data = pd.read_csv('Collection_Correct_Name_Scopus.csv')\n",
        "\n",
        "    # Mostrar los datos\n",
        "    st.subheader(\"Datos Previos\")\n",
        "    st.dataframe(data.head())\n",
        "\n",
        "    # Número de registros\n",
        "    st.write(\"Número de observaciones:\", data.shape[0])\n",
        "    st.write(\"Número de variables:\", data.shape[1])\n",
        "\n",
        "    # Estadísticas resumidas\n",
        "    st.write(\"Resumen estadístico:\")\n",
        "    st.write(data.describe(include='all'))\n",
        "\n",
        "    # Valores faltantes\n",
        "    st.write(\"Valores faltantes:\")\n",
        "    conteo_valores_faltantes = data.isnull().sum()\n",
        "    porcentaje_valores_faltantes = (conteo_valores_faltantes / len(data)) * 100\n",
        "\n",
        "    valores_faltantes_df = pd.DataFrame({\n",
        "        \"Columna\": conteo_valores_faltantes.index,\n",
        "        \"Conteo de Faltantes\": conteo_valores_faltantes.values,\n",
        "        \"Porcentaje de Faltantes\": [f\"{porcentaje:.2f}%\" for porcentaje in porcentaje_valores_faltantes.values]\n",
        "    })\n",
        "    st.dataframe(valores_faltantes_df)\n",
        "\n",
        "    # Análisis adicional de EDA y visualizaciones\n",
        "    st.subheader(\"Análisis de algunas variables relevantes\")\n",
        "\n",
        "    # Value counts para 'Patrocinadores'\n",
        "    st.write(\"Top 10 de los patrocinadores más frecuentes:\")\n",
        "    top_fund = data['fund_sponsor'].value_counts().head(10)\n",
        "    st.write(top_fund)\n",
        "\n",
        "    # Value counts para 'citedby_count'\n",
        "    st.write(\"Top 10 de autor con más citas:\")\n",
        "    top_cite_count = data.groupby('creator')['citedby_count'].size().sort_values(ascending=False).reset_index()\n",
        "    top_cite_count.columns = ['Autor', 'Conteo Citas']\n",
        "    top_cite_count = top_cite_count.head(10)\n",
        "    st.write(top_cite_count)\n",
        "\n",
        "    ## Value counts para 'Affiliations'\n",
        "    st.write(\"Top 10 de Afiliaciones más frecuentes:\")\n",
        "    top_Affiliations = data['affilname'].value_counts().head(10)\n",
        "    st.write(top_Affiliations)\n",
        "\n",
        "    # Value counts para 'country'\n",
        "    st.write(\"Top 10 de países más frecuentes:\")\n",
        "    top_country = data['affiliation_country'].value_counts().head(10)\n",
        "    st.write(top_country)\n",
        "\n",
        "    # Value counts para 'institution'\n",
        "    st.write(\"Top 10 de nombre de publicaciones más frecuentes:\")\n",
        "    top_institution = data['publicationName'].value_counts().head(10)\n",
        "    st.write(top_institution)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ5nyJrz8a-g",
        "outputId": "fe452961-b510-4e40-97b7-94b9b613a419"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing EDA_scopus.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def segment_names(names):\n",
        "    first_names = []\n",
        "    for name in names:\n",
        "        segments = name.split()\n",
        "        first_names.append(segments[0])\n",
        "\n",
        "    return first_names\n",
        "\n",
        "def generate_word_cloud(data):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(data)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    st.pyplot()\n",
        "\n",
        "def main():\n",
        "    st.title('Spanish Name Segmentation and Word Cloud')\n",
        "    st.write('Enter names with two names and two last names:')\n",
        "    names_input = st.text_area('Enter names (one per line)', height=200)\n",
        "    names = [name.strip() for name in names_input.split('\\n') if name.strip()]\n",
        "\n",
        "    if st.button('Segment Names'):\n",
        "        first_names = segment_names(names)\n",
        "        st.write('Segmented First Names:')\n",
        "        st.write('\\n'.join(first_names))\n",
        "\n",
        "        # Count male and female names\n",
        "        male_names = []\n",
        "        female_names = []\n",
        "        for name in first_names:\n",
        "            gender = input(\"Enter the gender of {} (M/F): \".format(name))\n",
        "            if gender.lower() == 'm':\n",
        "                male_names.append(name)\n",
        "            elif gender.lower() == 'f':\n",
        "                female_names.append(name)\n",
        "\n",
        "        # Calculate percentages\n",
        "        total = len(first_names)\n",
        "        male_percentage = len(male_names) / total * 100\n",
        "        female_percentage = len(female_names) / total * 100\n",
        "\n",
        "        # Generate word cloud\n",
        "        data = {'Male': male_percentage, 'Female': female_percentage}\n",
        "        generate_word_cloud(data)\n",
        "\n",
        "if __name__ == '_main_':\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fub-gViUiSqa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scopus Network"
      ],
      "metadata": {
        "id": "_aEL2I2_aY7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scopus_network.py\n",
        "\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvis.network import Network\n",
        "\n",
        "# Function to load the nodes and edges CSV files\n",
        "def load_data_network():\n",
        "    nodes_df = pd.read_csv('nodes.csv')\n",
        "    edges_df = pd.read_csv('edges.csv')\n",
        "    return nodes_df, edges_df\n",
        "\n",
        "# Function to create the network graph using NetworkX\n",
        "def create_graph(nodes_df, edges_df):\n",
        "    G = nx.Graph()\n",
        "    for _, row in nodes_df.iterrows():\n",
        "        G.add_node(row['Node'], ID=row['ID'])\n",
        "    for _, row in edges_df.iterrows():\n",
        "        G.add_edge(row['Source'], row['Target'])\n",
        "    return G\n",
        "\n",
        "# Function to get the first and second tier nodes for a given node\n",
        "def get_tiers(G, node):\n",
        "    first_tier = list(G.neighbors(node))\n",
        "    second_tier = []\n",
        "    for neighbor in first_tier:\n",
        "        second_tier.extend(list(G.neighbors(neighbor)))\n",
        "    second_tier = list(set(second_tier))  # Remove duplicates\n",
        "    return first_tier, second_tier\n",
        "\n",
        "# Create graph visualization based on selected attribute\n",
        "def visualization_graph(st):\n",
        "\n",
        "    # Load the data\n",
        "    nodes_df, edges_df = load_data_network()\n",
        "\n",
        "    # Create the graph\n",
        "    G = create_graph(nodes_df, edges_df)\n",
        "\n",
        "    # Dropdown to select a node\n",
        "    selected_node = st.selectbox(\"Seleccione el autor\", nodes_df['Node'])\n",
        "\n",
        "    # Get first and second tier nodes\n",
        "    first_tier_nodes, second_tier_nodes = get_tiers(G, selected_node)\n",
        "\n",
        "    # Subgraph containing the selected node and its first and second tier nodes\n",
        "    subgraph = G.subgraph([selected_node] + first_tier_nodes + second_tier_nodes)\n",
        "\n",
        "    # Visualization using pyvis\n",
        "    network = Network(width=\"100%\", height=\"800px\", notebook=True)\n",
        "    network.from_nx(subgraph)\n",
        "    network.show(\"network.html\")\n",
        "\n",
        "    # Display the network visualization\n",
        "    st.components.v1.html(open(\"network.html\", 'r', encoding='utf-8').read(), width=1000, height=600)\n",
        "\n",
        "    # Display first and second tier nodes\n",
        "    st.write(\"Coautores del investigador:\")\n",
        "    st.write(first_tier_nodes)\n",
        "    st.write(\"Coautores de los coautores:\")\n",
        "    st.write(second_tier_nodes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5jffL7dtd61",
        "outputId": "f3d1f3e0-fe77-458e-fcbb-5c283f8d4a42"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing scopus_network.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Afilliation Network"
      ],
      "metadata": {
        "id": "_0WNUV6j4ptH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile afi1_network.py\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvis.network import Network\n",
        "import csv\n",
        "import time\n",
        "\n",
        "\n",
        "# Function to load the nodes and edges CSV files\n",
        "def load_data_network():\n",
        "    nodes_df_afi = pd.read_csv('nodes60033370.csv')\n",
        "    edges_df_afi = pd.read_csv('edges60033370.csv')\n",
        "    return nodes_df_afi, edges_df_afi\n",
        "\n",
        "# Function to create the network graph using NetworkX\n",
        "def create_graph_afi(nodes_df_afi, edges_df_afi):\n",
        "    GA = nx.Graph()\n",
        "    for _, row in nodes_df_afi.iterrows():\n",
        "        GA.add_node(row['Node'], ID=row['ID'])\n",
        "    for _, row in edges_df_afi.iterrows():\n",
        "        GA.add_edge(row['Source'], row['Target'])\n",
        "    return GA\n",
        "\n",
        "# Function to get the first and second tier nodes for a given node\n",
        "def get_tiers(GA, node):\n",
        "    first_tier_afi = list(GA.neighbors(node))\n",
        "    second_tier_afi = []\n",
        "    for neighbor in first_tier_afi:\n",
        "        second_tier_afi.extend(list(GA.neighbors(neighbor)))\n",
        "    second_tier_afi = list(set(second_tier_afi))  # Remove duplicates\n",
        "    return first_tier_afi, second_tier_afi\n",
        "\n",
        "# Create graph visualization based on selected attribute\n",
        "def visualization_graph_afi(st):\n",
        "\n",
        "    # Load the data\n",
        "    nodes_df_afi, edges_df_afi = load_data_network()\n",
        "\n",
        "    # Create the graph\n",
        "    GA = create_graph_afi(nodes_df_afi, edges_df_afi)\n",
        "\n",
        "    # Dropdown to select a node\n",
        "    #selected_node = st.selectbox(\"Seleccione el autor\", nodes_df_afi['Node'])\n",
        "    selected_node = (\"Pontificia Universidad Javeriana\")\n",
        "\n",
        "    # Get first and second tier nodes\n",
        "    first_tier_afi_nodes, second_tier_afi_nodes = get_tiers(GA, selected_node)\n",
        "\n",
        "    # Subgraph containing the selected node and its first and second tier nodes\n",
        "    subgraph = GA.subgraph([selected_node] + first_tier_afi_nodes + second_tier_afi_nodes)\n",
        "\n",
        "    # Visualization using pyvis\n",
        "    network = Network(width=\"100%\", height=\"800px\", notebook=True)\n",
        "    network.from_nx(subgraph)\n",
        "    network.show(\"network_afi.html\")\n",
        "\n",
        "    # Display the network visualization\n",
        "    st.components.v1.html(open(\"network_afi.html\", 'r', encoding='utf-8').read(), width=1000, height=600)\n",
        "\n",
        "    # Display nodes different from the selected node\n",
        "    all_nodes = GA.nodes()\n",
        "    selected_node = set([selected_node])\n",
        "    different_nodes = set(all_nodes) - selected_node\n",
        "    st.write(\"universidades vinculadas\")\n",
        "    st.write(list(different_nodes))\n",
        "\n",
        "    # Export nodes to CSV if export button is clicked\n",
        "    export_button = st.button(\"Exportar a CSV\")\n",
        "    if export_button:\n",
        "        # CSV filename\n",
        "        csv_filename = \"VinculosUniversidades.csv\"\n",
        "\n",
        "        # Write different nodes to CSV file\n",
        "        with open(csv_filename, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Nodo\"])  # Write header\n",
        "            writer.writerows(zip(list(different_nodes)))  # Write nodes\n",
        "\n",
        "        # Success message\n",
        "        st.success(f\"Se ha exportado la lista de nodos diferentes a {csv_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYDR-HjjF9CP",
        "outputId": "80c22e2a-a98f-46fc-a7bf-4747314167a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing afi1_network.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custers"
      ],
      "metadata": {
        "id": "mX5sOu1sae1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cluster.py\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim import corpora\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def detect_communities(data, st):\n",
        "    st.subheader(\"Detección comunidades\")\n",
        "    # Load the data\n",
        "    df = data\n",
        "\n",
        "    # Select relevant columns for clustering\n",
        "    data = df[['Abstract', 'Author Keywords', 'Index Keywords', 'Authors']].dropna()\n",
        "\n",
        "    # Prepare the text corpus for word embeddings and LDA\n",
        "    corpus = data['Abstract'].tolist()\n",
        "\n",
        "    # Remove stop words from the corpus\n",
        "    stop_words = set(stopwords.words('english'))  # Use the appropriate language for your data\n",
        "    corpus = [[word for word in document.lower().split() if word not in stop_words] for document in corpus]\n",
        "\n",
        "    # Train Word2Vec model for word embeddings\n",
        "    model = Word2Vec(corpus, min_count=1)\n",
        "\n",
        "    # Extract word embeddings\n",
        "    word_embeddings = model.wv\n",
        "\n",
        "    # Define the number of topics\n",
        "    num_topics = 10\n",
        "\n",
        "    # Create a dictionary and corpus for LDA\n",
        "    dictionary = corpora.Dictionary(corpus)\n",
        "    corpus_lda = [dictionary.doc2bow(text) for text in corpus]\n",
        "\n",
        "    # Train LDA model\n",
        "    lda_model = LdaModel(corpus_lda, num_topics=num_topics, id2word=dictionary)\n",
        "\n",
        "    # Topic Visualization\n",
        "    topics = lda_model.show_topics(num_topics=num_topics, num_words=10)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(15, 8), sharex=True, sharey=True)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, topic in enumerate(topics):\n",
        "        words = dict(lda_model.show_topic(i, topn=10))\n",
        "        cloud = WordCloud(background_color='white').generate_from_frequencies(words)\n",
        "        axes[i].imshow(cloud, interpolation='bilinear')\n",
        "        axes[i].axis('off')\n",
        "        axes[i].set_title('Topic ' + str(i + 1))\n",
        "\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Display the topics with document labels\n",
        "    st.write(\"Topic Labels:\")\n",
        "    topic_labels = [lda_model[doc] for doc in corpus_lda]\n",
        "    \n",
        "    # Display the cluster labels with author names\n",
        "    cluster_data = pd.DataFrame({'Authors': data['Authors'], 'Topic': [max(doc, key=lambda x: x[1])[0] + 1 for doc in topic_labels]})\n",
        "    st.write(\"Topics for Documents:\")\n",
        "    st.write(cluster_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16A1JMPvv27K",
        "outputId": "1736482d-bb53-4fad-de22-20368fe25055"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cluster.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keyword_Analysis"
      ],
      "metadata": {
        "id": "nQBgHInnaj5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile keyword_analysis.py\n",
        "\n",
        "import streamlit as st\n",
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to generate a word cloud\n",
        "def generate_wordcloud(text, title, st):\n",
        "    stopwords_list = stopwords.words('english')\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = ' '.join([word for word in word_tokens if word.lower() not in stopwords_list])\n",
        "    \n",
        "    if filtered_text:\n",
        "        fig, ax = plt.subplots()\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(filtered_text)\n",
        "\n",
        "        ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "        ax.axis(\"off\")\n",
        "        ax.set_title(title)\n",
        "\n",
        "        st.pyplot(fig)\n",
        "\n",
        "def generate_keyword_analysis(data, st):\n",
        "\n",
        "    # Convert keyword values to strings\n",
        "    data['Author Keywords'] = data['Author Keywords'].astype(str)\n",
        "\n",
        "    # Split and select unique keywords\n",
        "    keywords = set(';'.join(data['Author Keywords']).split(';'))\n",
        "\n",
        "    # Select a keyword\n",
        "    keyword = st.selectbox(\"Palabras clave\", sorted(keywords))\n",
        "\n",
        "    # Filter the dataframe based on the keyword\n",
        "    if keyword:\n",
        "        filtered_data = data[data['Title'].str.contains(keyword, case=False) |\n",
        "                          data['Abstract'].str.contains(keyword, case=False) |\n",
        "                          data['Author Keywords'].str.contains(keyword, case=False)]\n",
        "\n",
        "        # Reset the index of the filtered data\n",
        "        filtered_data = filtered_data.reset_index(drop=True)\n",
        "        filtered_data = filtered_data.reset_index(drop=True)\n",
        "        filtered_data.index += 1\n",
        "\n",
        "        # Display word clouds\n",
        "        generate_wordcloud(' '.join(filtered_data['Title'].astype(str)), \"Nube de palabras por título\", st)\n",
        "        generate_wordcloud(' '.join(filtered_data['Abstract'].astype(str)), \"Nube de palabras por resumen\", st)\n",
        "        generate_wordcloud(' '.join(filtered_data['Author Keywords'].astype(str)), \"Nube de palabras por palabras clave\", st)\n",
        "\n",
        "        # Display the filtered dataframe\n",
        "        st.subheader(\"Artículos seleccionados\")\n",
        "        st.dataframe(filtered_data)\n",
        "    else:\n",
        "        st.info(\"Seleccionar una palabra clave.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNUPAE8sH2B_",
        "outputId": "4849a3af-881d-4f1b-9670-0ab5cc920bcd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing keyword_analysis.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ODS\n"
      ],
      "metadata": {
        "id": "do96nM4tapR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ODS.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "def ODS_analysis(data, st):\n",
        "\n",
        "  # Load the ODS dataframe\n",
        "  df_ods = pd.read_csv('output_OSD.csv')\n",
        "  df_ods = df_ods.drop(0)\n",
        "\n",
        "  # Load the data dataframe\n",
        "  df_all_data = data\n",
        "\n",
        "  # Create a selectbox to choose the \"Objetivo ODS\"\n",
        "  selected_objetivo_ods = st.selectbox(\"Selecciona un objetivo de desarrollo sostenible\", df_ods['Objetivo ODS'])\n",
        "\n",
        "  # Filter the data based on the selected \"Objetivo ODS\"\n",
        "  filtered_data = df_all_data[df_all_data['Title'].isin(df_ods[df_ods['Objetivo ODS'] == selected_objetivo_ods][['Titulo 1', 'Titulo 2', 'Titulo 3']].values.flatten())]\n",
        "\n",
        "  # Display the filtered data\n",
        "  st.write(filtered_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JjWcfWXBRNX",
        "outputId": "49436ec9-6e5f-42de-b573-4da5e2f69895"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ODS.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# H_Index"
      ],
      "metadata": {
        "id": "aPTNFBUBauAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile h_index.py\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "def get_authors(data):\n",
        "  \n",
        "  authors_list = data['Authors'].unique().tolist()\n",
        "\n",
        "  authors = []\n",
        "\n",
        "  for authors_values in authors_list:\n",
        "    authors.extend([item.strip() for item in authors_values.split(\"., \")])\n",
        "\n",
        "  palabras_conocidas = set(nltk.corpus.words.words())\n",
        "\n",
        "  # Identificar los nombres que no son palabras conocidas\n",
        "  nombres_sin_sentido = [nombre for nombre in authors if nombre.lower() not in palabras_conocidas]\n",
        "\n",
        "  return sorted(set(nombres_sin_sentido))\n",
        "\n",
        "\n",
        "def get_h_index(data, st):\n",
        "\n",
        "    authors_list = get_authors(data)\n",
        "\n",
        "    selected_node = st.selectbox(\"Seleccione el autor\", authors_list)\n",
        "\n",
        "    result = data.loc[data['Authors'].str.contains(selected_node, regex=False), 'Cited by'].tolist()\n",
        "\n",
        "    num_articles = len(result)\n",
        "\n",
        "    range_h = range(1, num_articles + 1)\n",
        "\n",
        "    citas_acumuladas = [sum(result[:i]) for i in range_h]\n",
        "\n",
        "    # Gráfico del índice H\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(range_h, citas_acumuladas, marker='o')\n",
        "    ax.set(xlabel='Número de artículos', ylabel='Citas acumuladas', title='Índice H')\n",
        "    st.pyplot(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy80gOgLPrFJ",
        "outputId": "a1ea0a83-0e41-4bb9-a1de-5b42dddafec5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing h_index.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Authors_top"
      ],
      "metadata": {
        "id": "fonbb9s9ayA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile authors_top.py\n",
        "import nltk\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_authors(data):\n",
        "  \n",
        "  authors_list = data['Authors'].unique().tolist()\n",
        "\n",
        "  authors = []\n",
        "\n",
        "  for authors_values in authors_list:\n",
        "    authors.extend([item.strip() for item in authors_values.split(\"., \")])\n",
        "\n",
        "  palabras_conocidas = set(nltk.corpus.words.words())\n",
        "\n",
        "  # Identificar los nombres que no son palabras conocidas\n",
        "  nombres_sin_sentido = [nombre for nombre in authors if nombre.lower() not in palabras_conocidas]\n",
        "\n",
        "  return sorted(nombres_sin_sentido)\n",
        "\n",
        "\n",
        "def get_authors_top(data, st):\n",
        "  numbers_list = [5, 10, 15, 20]\n",
        "  selected_node = st.selectbox(\"Seleccione cuantos autores quiere ver\", numbers_list)\n",
        "\n",
        "  authors = get_authors(data)\n",
        "  counts = {}\n",
        "\n",
        "  for item in authors:\n",
        "    if item in counts:\n",
        "      counts[item] += 1\n",
        "    else:\n",
        "      counts[item] = 1\n",
        "\n",
        "  sorted_count = dict(sorted(counts.items(), key=lambda x: x[1], reverse=True))\n",
        "  top = dict(itertools.islice(sorted_count.items(), selected_node))\n",
        "\n",
        "  autores = list(top.keys())\n",
        "  conteos = list(top.values())\n",
        "\n",
        "  # Crear la figura y el gráfico de barras\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.bar(autores, conteos)\n",
        "\n",
        "  # Personalizar el gráfico\n",
        "  ax.set(xlabel='Autores', ylabel='Conteo', title='Top de Autores')\n",
        "\n",
        "  # Rotar las etiquetas del eje x para mayor legibilidad\n",
        "  plt.xticks(rotation=45)\n",
        "\n",
        "  # Mostrar el gráfico en Streamlit\n",
        "  st.pyplot(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZPEWOgoPvMD",
        "outputId": "f883f973-6e5e-43c3-ce9b-d461cea4d0a9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing authors_top.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-np2eLhOnrV",
        "outputId": "886c6b52-d27f-477c-b694-12bf21b3400e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-b793247e3477>:7: FutureWarning: The geopandas.dataset module is deprecated and will be removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/.\n",
            "  world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "world['name']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7_4zCNMOr2L",
        "outputId": "56033402-83f4-4160-c27f-a178c1ae858e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                          Fiji\n",
              "1                      Tanzania\n",
              "2                     W. Sahara\n",
              "3                        Canada\n",
              "4      United States of America\n",
              "                 ...           \n",
              "172                      Serbia\n",
              "173                  Montenegro\n",
              "174                      Kosovo\n",
              "175         Trinidad and Tobago\n",
              "176                    S. Sudan\n",
              "Name: name, Length: 177, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Countries"
      ],
      "metadata": {
        "id": "yI_EQcRHl6NM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile plot_countries.py\n",
        "\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "def countries_plot():\n",
        "    # Load world shapefile\n",
        "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
        "\n",
        "    # Convert country column to a list of lists\n",
        "    countries = data['country'].fillna('').apply(lambda x: list(set(str(x).split(';'))))\n",
        "\n",
        "    # Flatten the nested lists into a single list\n",
        "    flat_list = [item for sublist in countries for item in sublist]\n",
        "\n",
        "    # Count the occurrences of each country\n",
        "    country_counts = Counter(flat_list)\n",
        "\n",
        "    # Rename \"United States\" to \"United States of America\" in the country counts\n",
        "    if \"United States\" in country_counts:\n",
        "        country_counts[\"United States of America\"] = country_counts.pop(\"United States\")\n",
        "\n",
        "    # Merge country counts with world shapefile\n",
        "    world['Publication Count'] = world['name'].map(country_counts)\n",
        "\n",
        "    # Sort countries by count in descending order\n",
        "    sorted_countries = world.sort_values('Publication Count', ascending=False)\n",
        "\n",
        "    # Get top 10 countries\n",
        "    top_10_countries = sorted_countries.head(10)\n",
        "\n",
        "    # Plot the map\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    top_10_countries.plot(column='Publication Count', cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
        "\n",
        "    # Set plot title\n",
        "    ax.set_title('Distribución geográfica de las publicaciones')\n",
        "\n",
        "    # Remove axis ticks and labels\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    # Display the map in Streamlit\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Display the table\n",
        "    st.subheader('Top 10 de cantidad de publicaciones por países')\n",
        "    st.dataframe(top_10_countries[['name', 'Publication Count']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZznNPesqmBIt",
        "outputId": "0ff57be9-9b5c-4a53-bcfb-354499367527"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing plot_countries.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cities\n"
      ],
      "metadata": {
        "id": "cu8fIfwYW8sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile plot_cities.py\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import streamlit as st\n",
        "\n",
        "def plot_colombia_cities():\n",
        "\n",
        "    data = pd.read_csv('Collection_Correct_Name_Scopus.csv')\n",
        "\n",
        "    # Filter data for Colombia\n",
        "    colombia_data = data[data['affiliation_country'] == 'Colombia']\n",
        "\n",
        "    # Split affiliation cities and convert to a list of lists\n",
        "    cities = colombia_data['affiliation_city'].fillna('').apply(lambda x: list(set(str(x).split(';'))))\n",
        "\n",
        "    # Flatten the nested lists into a single list\n",
        "    flat_list = [item for sublist in cities for item in sublist]\n",
        "\n",
        "    # Count the occurrences of each city\n",
        "    city_counts = Counter(flat_list)\n",
        "\n",
        "    # Convert city_counts into a DataFrame\n",
        "    city_counts_df = pd.DataFrame.from_dict(city_counts, orient='index', columns=['Count'])\n",
        "\n",
        "    # Sort the DataFrame by city count in descending order\n",
        "    city_counts_df = city_counts_df.sort_values('Count', ascending=False)\n",
        "\n",
        "    # Reset the index of the DataFrame\n",
        "    city_counts_df = city_counts_df.reset_index().rename(columns={'index': 'City'})\n",
        "\n",
        "    # Display the table\n",
        "    st.subheader('Publicaciones por ciudades')\n",
        "    st.dataframe( city_counts_df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAasTEECW_m9",
        "outputId": "c9253108-a53b-430a-b512-69265d84b152"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting plot_cities.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main APP"
      ],
      "metadata": {
        "id": "BpvTpIiujqxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "#Features\n",
        "from EDA import perform_eda\n",
        "from scopus_network import visualization_graph\n",
        "from afi1_network import visualization_graph_afi\n",
        "from cluster import detect_communities\n",
        "from keyword_analysis import generate_keyword_analysis\n",
        "from ODS import ODS_analysis\n",
        "from h_index import get_h_index\n",
        "from authors_top import get_authors_top\n",
        "from plot_countries import countries_plot\n",
        "from EDA_proy_inv import perform_eda_proy_inv\n",
        "from EDA_scopus import perform_eda_scopus\n",
        "from plot_cities import plot_colombia_cities\n",
        "\n",
        "    \n",
        "def main():\n",
        "    st.title(\"Redes de investigación de la PUJ\")\n",
        "    \n",
        "    # Read the CSV file\n",
        "    data = pd.read_csv('papersPreprocessed.csv')\n",
        "\n",
        "    # Sidebar selection\n",
        "    selection = st.sidebar.radio(\"Selecciona una opción\",\n",
        "     (\"Exploración de datos de Scopus\",\n",
        "      \"Exploración de datos de WoS y Scopus\",\n",
        "     \"Exploración de Proyectos de Investigación\",\n",
        "     \"Redes de Investigadores\",\n",
        "     \"Red de afiliaciones de la PUJ\",\n",
        "      \"Segmentación de artículos\",\n",
        "      \"Análisis de palabras claves\",\n",
        "      \"Relación de ODS con artículos\",\n",
        "      \"Indice H de citaciones\",\n",
        "      \"Top de autores con más publicaciones\",\n",
        "      \"Visualización geográfica por países\", \n",
        "      \"Visualización geográfica por ciudades de Colombia\"))\n",
        "    if selection == \"Exploración de datos de WoS y Scopus\":\n",
        "        st.subheader(\"Conoce aspectos relevantes de los datos de investigadares\")\n",
        "        perform_eda(data, st)\n",
        "    elif selection == \"Redes de Investigadores\":\n",
        "        st.subheader(\"Exploremos la red de autores y coautores de Scopus\")\n",
        "        visualization_graph(st)\n",
        "    elif selection == \"Red de afiliaciones de la PUJ\":\n",
        "        st.subheader(\"Exploremos las afiliaciones de la PUJ\")        \n",
        "        visualization_graph_afi(st)        \n",
        "    elif selection == \"Segmentación de artículos\":\n",
        "        st.subheader(\"Definamos algunos grupos de artículos\")  \n",
        "        detect_communities(data, st)\n",
        "    elif selection == \"Análisis de palabras claves\":\n",
        "        st.subheader(\"Revisemos las palabras claves y sus artículos relacionados\")\n",
        "        generate_keyword_analysis(data, st)\n",
        "    elif selection == \"Relación de ODS con artículos\":\n",
        "        st.subheader(\"¿Cuál objetivo quieres revisar?\")\n",
        "        ODS_analysis(data, st)\n",
        "    elif selection == \"Indice H de citaciones\":\n",
        "        st.subheader(\"Revisemos el índice de citaciones\")\n",
        "        get_h_index(data, st)\n",
        "    elif selection == \"Top de autores con mas publicaciones\":\n",
        "        st.subheader(\"Revisemos el top de autores con más publicaciones\")\n",
        "        get_authors_top(data, st)\n",
        "    elif selection == \"Visualización geográfica\":\n",
        "        st.subheader(\"Revisemos el tema a nivel mundial\")\n",
        "        countries_plot(data, st)\n",
        "    elif selection == \"Exploración de Proyectos de Investigación\":\n",
        "        st.subheader(\"Revisemos el tema a nivel mundial\")\n",
        "        perform_eda_proy_inv(st)\n",
        "    elif selection == \"Exploración de datos de Scopus\":\n",
        "        st.subheader(\"Conoce aspectos relevantes de los datos de investigadares de Scopus\")\n",
        "        perform_eda_scopus(st)\n",
        "    elif selection == \"Visualización geográfica por ciudades de Colombia\":\n",
        "        st.subheader(\"Revisemos el tema en Colombia\")\n",
        "        plot_colombia_cities()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZiKTZhzyKst",
        "outputId": "2b8def3d-31cd-4bf5-954c-88e17e183618"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "Ws-K0BZ4C9fH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Frr7dCiDCSf",
        "outputId": "22938fc0-2997-4339-bfc6-0f0bce7b286f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mcli\u001b[0m npm v9.6.7 does not support Node.js v14.16.0. This version of npm supports the following node versions: `^14.17.0 || ^16.13.0 || >=18.0.0`. You can find the latest version at https://nodejs.org/.\n",
            "\u001b[0myour url is: https://empty-falcons-smell.loca.lt\n"
          ]
        }
      ]
    }
  ]
}